= Aggregating Container Logs
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

As an OpenShift administrator, you may wish to view the logs from all containers
in one user interface. There are two options for aggregating logs, depending on
user requirements:

. <<Using a Centralized Filesystem>>
. <<Using Elasticsearch>>

IMPORTANT: These solutions are a work in progress. As packaging improvements are
made these instructions will be simplified.

---

= Using a Centralized Filesystem

This option will read all container logs & forward them to a central server
for storage on the filesystem. This solution requires less resources &
require less management than the <<Using Elasticsearch,Elasticsearch>> option, but
the logs will not be indexed & searchable.

== Installing fluentd (td-agent) on Nodes

[source,sh]
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl http://packages.treasuredata.com/2/redhat/7/x86_64/$RPM -o /tmp/$RPM
# yum localinstall $RPM
# /opt/td-agent/embedded/bin/gem install fluent-plugin-kubernetes
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----

=== Configuring fluentd to Read Container Logs

Create the *_/etc/sysconfig/td-agent_* file with the following contents:

[source,sh]
----
DAEMON_ARGS=
TD_AGENT_ARGS="/usr/sbin/td-agent --log /var/log/td-agent/td-agent.log --use-v1-config"
----

This is required to allow *td-agent* access to the containers logs.

Add the following line to the *_/etc/td-agent/td-agent.conf_* file:

----
  @include config.d/*.conf
----

Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:

[source,xml]
----
    <source>
      type tail
      path /var/lib/docker/containers/*/*-json.log
      pos_file /var/log/td-agent/tmp/fluentd-docker.pos
      time_format %Y-%m-%dT%H:%M:%S
      tag docker.*
      format json
      read_from_head true
    </source>

    <match docker.var.lib.docker.containers.*.*.log>
      type kubernetes
      container_id ${tag_parts[5]}
      tag docker.${name}
    </match>

    <match kubernetes>
      type copy
      <store>
        type forward
        send_timeout 60s
        recover_wait 10s
        heartbeat_interval 1s
        phi_threshold 16
        hard_timeout 60s
        log_level trace
        require_ack_response true
        heartbeat_type tcp
        <server>
          name logging_name       <!--1-->
          host host_name          <!--2-->
          port 24224
          weight 60
        </server>

        <secondary>
          type file
          path /var/log/td-agent/forward-failed
        </secondary>
      </store>

      <store>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
      </store>
    </match>
----
<1> The name for the master that will be used during logging.
<2> The IP or a DNS resolvable name used to access the master.

=== Enabling fluentd

[source,sh]
----
# systemctl enable td-agent
# systemctl start td-agent
----

TIP: Any errors will be logged in the *_/var/log/td-agent/td-agent.log_* file.

=== Optional Method to Verify Working Nodes

You can optionally set up the master to be the aggregator to test and verify
that the nodes are working properly.

*Installing fluentd (td-agent) on the Master*

[source,sh]
----
# export RPM=td-agent-2.2.0-0.x86_64.rpm
# curl http://packages.treasuredata.com/2/redhat/7/x86_64/$RPM -o /tmp/$RPM
# yum localinstall $RPM
# mkdir -p /etc/td-agent/config.d
# chown td-agent:td-agent /etc/td-agent/config.d
----

Ensure port 24224 is open on the master's firewall to allow the nodes access.

*Configuring fluentd to Aggregate Container Logs*

Add the following line to the *_/etc/td-agent/td-agent.conf_* file:

----
  @include config.d/*.conf
----

Create the *_/etc/td-agent/config.d/kubernetes.conf_* file with the following
contents:

[source,html]
----
    <match kubernetes.**>
        type file
        path /var/log/td-agent/containers.log
        time_slice_format %Y%m%d
        time_slice_wait 10m
        time_format %Y%m%dT%H%M%S%z
        compress gzip
        utc
    </match>
----

*Enabling fluentd*

[shell,sh]
****
    # systemctl enable td-agent
    # systemctl start td-agent
****

TIP: Any errors will be logged in the *_/var/log/td-agent/td-agent.log_* file.

You should now find all the containers' logs available on the master in the
*_/var/log/td-agent/containers.log_* file.

---

= Using Elasticsearch

https://www.elastic.co/products/elasticsearch[Elasticsearch] is an open source
distributed document database that indexes documents & provides full-text
search capabilities. By storing container logs in Elasticsearch, users will be
able to search all content & filter appropriately. This documentation shows how
to run https://www.elastic.co/products/kibana[Kibana].

This option requires more configuration & more resources than the
<<Using a Centralized Filesystem,centralised filesystem>> option, but makes logs more useful
for troubleshooting & fault finding.

Enabling aggregated logging to Elasticsearch involves:

. <<Creating an Elasticsearch cluster>>
. <<Creating logging pods>>
. <<Creating Kibana service>>

== Creating an Elasticsearch cluster

Logs are stored in an Elasticsearch cluster running on OpenShift. This cluster
is scalable via a replication controller so you can scale the Elasticsearch
cluster up & down as required.

The following is the manifest for the Elasticsearch cluster:

[source,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging"
  spec:
    ports:
    -
      port: 9200
      targetPort: 9200
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging-cluster"
  spec:
    portalIP: "None"
    ports:
    -
      port: 9300
      targetPort: 9300
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "elasticsearch"
  spec:
    replicas: 1
    selector:
      provider: "fabric8"
      component: "elasticsearch"
    template:
      metadata:
        labels:
          provider: "fabric8"
          component: "elasticsearch"
      spec:
        containers:
          -
            env:
            -
              name: "KUBERNETES_TRUST_CERT"
              value: "true"
            -
              name: "SERVICE_DNS"
              value: "es-logging-cluster"
            image: "fabric8/elasticsearch-k8s:1.5.2"
            name: "elasticsearch"
            ports:
            -
              containerPort: 9200
              name: "http"
            -
              containerPort: 9300
              name: "transport"
----

Save this to a file & create with:

```
oc create -f path/to/elasticsearch.yaml
```

This will start a single Elasticsearch instance. If you need to create a larger
cluster you can resize the Elasticsearch replication controller via:

```
oc resize --replicas=3 rc elasticsearch
```

## Creating logging pods

In order to read the container logs, a static pod is deployed on each node.
To do this you must first ensure that the node is configured to read local pod
manifest config files. This is enabled by configuring the `podManifestConfig`
in the `node-config.yaml` file, changing the config path & check interval
appropriately:

[source,yaml]
----
podManifestConfig:
  path: openshift.local.manifests
  fileCheckIntervalSeconds: 10
----

To create the logging pod, create a file with the following contents in the
`podManifestConfig.path` directory:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
spec:
  containers:
  - name: fluentd-elasticsearch
    image: fabric8/fluentd-kubernetes:1.0
    privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
----

This will start a pod on the node & post the container logs to Elasticsearch.

## Validate

To validate it is working, we will query Elasticsearch & check that the data is
correctly being persisted. First, we need to identify one of the Elasticsearch
pods:

[source,bash]
----
oc get pods -l component=elasticsearch
----

And then we can query Elasticsearch, replacing the pod ID with one returned
from the above call:

[source,bash]
----
oc exec -p <podID> -c elasticsearch -- curl -s localhost:9200/_cat/indices?v
----

You should see output similar to the following:

```
health status index               pri rep docs.count docs.deleted store.size pri.store.size
yellow open   logstash-2015.06.05   5   1        540            0      251kb          251kb
```

If the value for `docs.count` is more than 0, then log records are being sent
to Elasticsearch & you're all set.

== Creating Kibana service

To create the Kibana service save the following spec to your filesystem:

[souce,yaml]
----
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    name: "kibana"
  spec:
    ports:
      -
        port: 80
        targetPort: "kibana-port"
    selector:
      provider: fabric8
      component: "kibana"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    name: "kibana"
    labels:
      provider: fabric8
      component: "kibana"
  spec:
    replicas: 1
    selector:
      component: "kibana"
    template:
      metadata:
        name: "kibana"
        labels:
          provider: fabric8
          component: "kibana"
      spec:
        containers:
          -
            name: "kibana"
            image: "fabric8/kibana4:4.0.2"
            ports:
              -
                name: "kibana-port"
                containerPort: 5601
            env:
              -
                name: "ELASTICSEARCH_URL"
                value: "http://es-logging:9200"
----

And create the Kibana replication controller & service via:

[source,bash]
----
oc create -f path/to/kibana.yaml
----
