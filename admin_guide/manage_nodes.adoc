= Managing Nodes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
As an OpenShift administrator, you can manage
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]
in your instance using the link:../cli_reference/overview.html[CLI].

When you perform node management operations, the CLI interacts with
link:../architecture/infrastructure_components/kubernetes_infrastructure.html#node[node
objects] that are representations of nodes. The master uses the information from
node objects to validate nodes with health checks.

== Listing Nodes
Use the following command to list all nodes that are known to your OpenShift
instance:

****
`$ oc get nodes`
****

.Listing All Nodes
====

----
$ oc get nodes
NAME                 LABELS              STATUS
node1.example.com    <none>              Ready
node2.example.com    <none>              NotReady
----
====

Use the following command to list information about a single node, replacing
`_<node>_` with the full node name:

****
`$ oc get node _<node>_`
****

The `STATUS` column in the output of these commands indicates whether the node
is passing the health checks performed from the master. Nodes can be shown in
this column with the following conditions:

.Node Conditions
[cols="3a,8a",options="header"]
|===

|Condition |Description

|`Ready`
|The node has returned `StatusOK` for the health check.

|`NotReady`
|The health check is not passing.
|===

NOTE: The `STATUS` column can also show `Unknown` for a node if the CLI cannot
find any node condition.

Use the following command to get more detailed information about a node,
including the reason for the current condition:
****
`$ oc describe node _<node>_`
****

.Getting a Node Description
====

[options="nowrap"]
----
$ oc describe node node2.example.com
Name:	node2.example.com
Conditions:
Kind    Status  LastProbeTime                   LastTransitionTime              Reason                                                              Message
Ready   None    Tue, 03 Mar 2015 22:53:22 +0000 Tue, 03 Mar 2015 21:27:50 +0000 Node health check failed: kubelet /healthz endpoint returns not ok
----
====

== Adding a Node
You can add a node to an existing OpenShift instance using the `oc create`
command and supplying a manifest for a node object. After the new node passes
the health checks that are performed by the master,  pods can begin to be
scheduled for the node.

.To Add a Node to an Existing OpenShift Instance:
. Create a file with the following JSON format:
+
.*_node.json_* file
====

----
{
  "id": "node3.example.com", <1>
  "kind": "Node", <2>
  "apiVersion": "v1beta1", <3>
  "labels": { <4>
    "name": "node3",
  },
}
----

<1> Set the *`id`* parameter to the fully-qualified domain name where the node can be reached.
This value will be shown in the `NAME` column when running the `oc get nodes`
command.
<2> Set the *`kind`* parameter to `Node` to identify this is a manifest for a node
object.
<3> Set the *`apiVersion`* parameter to the Kubernetes API version you are using.
<4> Set any desired labels using the *`labels`* resource.
====

. Create a node object using the file you created:
+
====

----
$ oc create -f node.json
----
====
+
At this point, the master begins validating the new node by performing health
checks.

. Verify that the node was added by checking the output of the following
command:
+
====

----
$ oc get nodes
NAME                 LABELS              STATUS
node1.example.com    <none>              Ready
node2.example.com    <none>              Ready
node3.example.com    <none>              Ready
----
====
+
Pods are not scheduled for the node until the node passes the health checks that
are performed by the master. When the node passes these health checks, it is
then placed in the `Ready` condition.

== Deleting a Node
Use the following command to delete a node:

****
`$ oc delete node _<node>_`
****

NOTE: When you delete a node with the CLI, although the node object is deleted
in Kubernetes, the pods that exist on the node itself are not deleted. However,
the pods cannot be accessed by OpenShift. The behavior around deleting nodes and
pods with the CLI is under active development.

== Listing Pods on Nodes
Use the following command to list all or selected pods on one or more nodes:

****
`$ oadm manage-node _<node1>_ _<node2>_ --list-pods [--pod-selector=_<pod_selector>_] [-o json|yaml]`
****

To list all or selected pods on selected nodes:

****
`$ oadm manage-node --selector=_<node_selector>_ --list-pods [--pod-selector=_<pod_selector>_] [-o json|yaml]`
****

== Marking Nodes as Unschedulable or Schedulable
Use the following command to mark the nodes as unschedulable:

****
`$ oadm manage-node _<node1>_ _<node2>_ --schedulabe=false`
****

Marking node as unschedulable will block any new pods to be scheduled on the
node. Existing pods on the node will not get affected.

Use the following command to mark the nodes as schedulable:

****
`$ oadm manage-node _<node1>_ _<node2>_ --schedulabe` or

`$ oadm manage-node _<node1>_ _<node2>_ --schedulabe=true`
****

Marking node as schedulable will allow any new pods to be scheduled on the
node.

Instead of specifying the nodes _<node1>_ _<node2>_, you can also use
--selector=_<node_selector>_ to mark selected nodes as schedulabe or
unschedulable.

== Evacuating Pods on Nodes
Use the following command to evacuate all or selected pods on one or more nodes:

****
`$ oadm manage-node _<node1>_ _<node2>_ --evacuate [--pod-selector=_<pod_selector>_]`
****

Nodes must be marked unschedulable to perform pod evacuation. Only pods backed
by replication controller will be evacuated. Bare pods will not be touched.
You can force deletion of bare pods by using --force option:

****
`$ oadm manage-node _<node1>_ _<node2>_ --evacuate --force [--pod-selector=_<pod_selector>_]`
****

You can list pods that will be migrated by using --dry-run option:

****
`$ oadm manage-node _<node1>_ _<node2>_ --evacuate --dry-run [--pod-selector=_<pod_selector>_]`
****

Instead of specifying the nodes _<node1>_ _<node2>_, you can also use
--selector=_<node_selector>_ to evacuate pods on selected nodes.
