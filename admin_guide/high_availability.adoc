= High Availability
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
This document describes how to make a network service highly available using virtual IP addresses and IP failover. Even though we specifically describe an example of how to setup a highly available router environment, it can be generically applied to any IP based service.

== Why IP Failover?
Within a cluster, the Kubernetes replication controller ensures that the
deployment requirements - in particular the number of replicas - are satisfied when the appropriate resources are available.

The link:../architecture/core_objects/routing.html[Router] is one such deployment which can be run with 2 or more replicas to provide a highly available service resilient to individual router instance failures. Depending on how these router instances are discovered (for example via a well-published service or a DNS entry or even IP addresses), this could well impose operational requirements to handle failure cases when 1 or more router instances are "unreachable".

For some IP based traffic services (our example being the router), if we can ensure that the IP addresses are always serviced as long as a single instance is available, this simplifies the operational overhead and handles failure cases gracefully. It is pertinent to note here, that even though the service is highly available, performance could still be impacted.

== Use Cases
  1. As an administrator, I want my cluster to be assigned a resource set
     and I want the cluster to automatically manage those resources.
  2. As an administrator, I want my cluster to be assigned a set of virtual
     IP addresses that the cluster manages and migrates (with zero or
     minimal downtime) on failure conditions.
  3. As an addendum to use case #2, the administrator should not be
     required to perform any manual interaction to update the upstream
     "discovery" sources (e.g. DNS). The cluster should service all the
     assigned virtual IPs when atleast a single node is available - and
     this should be inspite of the fact that the current available
     resources are not sufficient to reach "critical mass" aka the
     desired state.

== ipfailover admin command
The `oscadmin ipfailover` command helps to setup the VIP failover configuration. An administrator can configure IP failover on an entire cluster or as would normally be the case on a subset of nodes (as defined via a labelled selector). If you are running in production, it is recommended that the labelled selector for the nodes matches atleast 2 nodes to ensure you have failover protection and that you provide a --replicas=<n> value that matches the number of nodes for the given labelled selector.

Running with the `--help` option will show you the supported options and their description. A small subset of these options pertinent to this document is described in the next section.

****
`$ oscadm ipfailover --help`
****

=== ipfailover command syntax

        oscadm ipfailover [<name>] <options>
        where:
            <name> = Name of the IP failover configuration.
                     Default: generated name (e.g.  ipfailover-1)
            <options> = One or more options - see subset below.

=== ipfailover command options (subset)
The list of command options described here are a subset that are relevant to this document.

            <options> = One or more of:
                --create
                --credentials=<credentials>
                -l,--selector=<selector>
                --virtual-ips=<ip-range>
                -i|--interface=<interface>
                -w|--watch-port=<port>

            <credentials> = <string> - Path to .kubeconfig file containing credentials to use to contact the master.
            <selector> = <string> - The node selector to use for running the HA sidecar pods.
            <ip-range> = string - One or more comma separated IP address or ranges.
                                  Example: 10.2.3.42,10.2.3.80-84,10.2.3.21
            <interface> = <string> - The interface to use.
                                     Default: Default interface on node or eth0
            <port> = <number> - Port to watch for resource availability.
                                Default: 80.
            <string> = a string of characters.
            <number> = a number ([0-9]*).


== Setting up Routers with IP Failover
The following steps describe how to setup a highly available router environment with IP failover in a 3-step operation:

=== Step 1: Label the infrastructures nodes for the service (router)
Strictly speaking, this step can be optional as you can run the router instances on any of the nodes in your Kubernetes cluster and use Virtual IP addresses (VIPs) that can "float" within those nodes.

However that said, it is recommended you provision certain infrastructure nodes to run the routers and have VIPs that can "float" amongst these nodes. In a complex and possibly bigger cluster, you probably may be already doing something similar so that nodes may be filtered on constraints or requirements specified (e.g. nodes with SSD drives or higher cpu/memory/disk requirements, etc).

In our example, let us define this label or constraint as router instances servicing traffic in the US west geography "ha-router=geo-us-west".

****
`$ openshift kube label nodes openshift-minion-{5,6,7,8,9} "ha-router=geo-us-west"`
****

=== Step 2: Run the IP service (router) with 2 or more replicas
As described in the link:../architecture/core_objects/routing.html[Router]
documentation, start the router with atleast 2 replicas on nodes matching the constraints or label we used in step 1. In our example, we are going to run 3 instances.
It is worth noting here that we are running a lesser number of replicas for the router than available nodes. This is so that in the case of node failures, Kubernetes will still be able ensure that we have 3 instances available - until of course the number of available "ha-router=geo-us-west" nodes is below 3.

****
`$ oscadm router ha-router-us-west --replicas=3 --labels="ha-router=geo-us-west" --credentials="$OPENSHIFTCONFIG" --create`
****

=== Step 3: Configure IP failover for the service (router)
The final step is to configure the virtual IPs and failover for the nodes labelled in step 1 (with "ha-router=geo-us-west"). Ensure the number of replicas matches the number of nodes that satisfy the constraint or label we used in step 1. Specify the virtual IP address and that the IP failover should monitor port 80 (the router) on those instances.

****
`$ oscadm ipfailover ha-router-us-west --replicas=5 --selector="ha-router=geo-us-west" --virtual-ips=10.245.2.101-105" --watch-port=80 --credentials="$OPENSHIFTCONFIG" --create`
****

== Internals
The oscadm ipfailover command ensures that an ipfailover sidecar pod runs on each of the nodes matching the constraints or label we used in step 1. This ipfailover sidecar pod uses VRRP (Virtual Router Reduncy Protocol) within keepalived to monitor that the service on the watched port is available and keepalived will automatically float the VIPs in event of the service not being available.

In our example, we can now use the VIPs 10.245.2.101 through 10.245.2.105 to send traffic to the routers. If a particular router instance is "unreachable" (as an example due to a node failure), keepalived will float the VIPs automatically float amongst group of nodes we labelled "ha-router=geo-us-west" and the router would still be reachable via those virtual IP addresses.

== Examples

See what the IP failover configuration would look like if it is created:

****
`$ oscadm ipfailover <options ...> -o json`
****


Create an IP failover configuration if it does not already exist:

****
`$ oscadm ipfailover --virtual-ips="1.2.3.4-5,6.7.8.9,10.11.12.13-15" --create`
****


Create an IP failover configuration on a selection of nodes labelled "router=har" (on 4 nodes with 7 virtual IPs monitoring a service listening on port 80 (aka the OpenShift router process).

****
`$ oscadm ipfailover ipfailover --selector="router=har" --virtual-ips="10.245.2.42,10.245.2.100-104,10.245.2.142,10.245.2.242" --watch-port=80 --replicas=4 --create`
****
