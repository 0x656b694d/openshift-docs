= High Availability
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
This document describes how to setup high available services on your
OpenShift cluster.

== IP Failover
This section describes how to make any IP based network service highly available using virtual IP addresses and IP failover.

=== Why IP Failover?
Within a cluster, the Kubernetes replication controller ensures that the deployment requirements - in particular the number of replicas - are satisfied when the appropriate resources are available.

The link:../architecture/core_objects/routing.html[Router] is one such deployment which can be run with 2 or more replicas to provide a highly available service resilient to individual router instance failures. Depending on how these router instances are discovered (for example via a well-published service or a DNS entry or even IP addresses), this could well impose operational requirements to handle failure cases when 1 or more router instances are "unreachable".

For some IP based traffic services, if we can ensure that the IP addresses are always serviced as long as a single instance is available, this simplifies the operational overhead and handles failure cases gracefully. It is pertinent to note here, that even though the service is highly available, performance could still be impacted.

=== Use Cases
  1. As an administrator, I want my cluster to be assigned a resource set
     and I want the cluster to automatically manage those resources.
  2. As an administrator, I want my cluster to be assigned a set of virtual
     IP addresses that the cluster manages and migrates (with zero or
     minimal downtime) on failure conditions.
  3. As an addendum to use case #2, the administrator should not be
     required to perform any manual interaction to update the upstream
     "discovery" sources (e.g. DNS). The cluster should service all the
     assigned virtual IPs when atleast a single node is available - and
     this should be inspite of the fact that the current available
     resources are not sufficient to reach "critical mass" aka the
     desired state.

=== ipfailover admin command
The `osadm ipfailover` command helps to setup the VIP failover configuration. An administrator can configure IP failover on an entire cluster or as would normally be the case on a subset of nodes (as defined via a labelled selector). If you are running in production, it is recommended that the labelled selector for the nodes matches atleast 2 nodes to ensure you have failover protection and that you provide a --replicas=<n> value that matches the number of nodes for the given labelled selector.

Running with the `--help` option will show you the supported options and their description. A small subset of these options pertinent to this document is described in the next section.

****
`$ osadm ipfailover --help`
****

==== ipfailover command syntax

        osadm ipfailover [<name>] <options>
        where:
            <name> = Name of the IP failover configuration.
                     Default: generated name (e.g.  ipfailover-1)
            <options> = One or more options - see subset below.

==== ipfailover command options (subset)
The list of command options described here are a subset that are relevant to this document.

            <options> = One or more of:
                --create
                --credentials=<credentials>
                -l,--selector=<selector>
                --virtual-ips=<ip-range>
                -i|--interface=<interface>
                -w|--watch-port=<port>

            <credentials> = <string> - Path to .kubeconfig file containing credentials to use to contact the master.
            <selector> = <string> - The node selector to use for running the HA sidecar pods.
            <ip-range> = string - One or more comma separated IP address or ranges.
                                  Example: 10.2.3.42,10.2.3.80-84,10.2.3.21
            <interface> = <string> - The interface to use.
                                     Default: Default interface on node or eth0
            <port> = <number> - Port to watch for resource availability.
                                Default: 80.
            <string> = a string of characters.
            <number> = a number ([0-9]*).

==== Setting up an IP based network service with IP Failover
The following steps describe how to setup a highly available IP based network service with IP failover in a 3-step operation:

===== Step 1: Label the nodes for the service
Strictly speaking, this step can be optional as you can run the service on any of the nodes in your Kubernetes cluster and use Virtual IP addresses (VIPs) that can "float" within those nodes.

However that said, it is recommended you provision certain nodes to run the service and have VIPs that can "float" amongst these nodes. In a complex and possibly bigger cluster, you probably may be already doing something similar so that nodes may be filtered on constraints or requirements specified (e.g. nodes with SSD drives or higher cpu/memory/disk requirements, etc).

In our example, let us define this label or constraint as "ha-cache=geo" for our highly available cache service binding/listening on port 9736.

****
`$ openshift kube label nodes openshift-minion-{6,3,7,9} "ha-cache=geo"`
****

===== Step 2: Run the geo cache service with 2 or more replicas
An example configuration for running a
https://raw.githubusercontent.com/openshift/openshift-docs/master/admin_guide/examples/geo-cache.json[geo-cache]
service is provided along with this document. The docker image referenced in
that file `myimages/geo-cache` does not exist and is there purely for reference
purposes. Edit the
https://raw.githubusercontent.com/openshift/openshift-docs/master/admin_guide/examples/geo-cache.json[geo-cache.json]
configuration to replace `myimages/geo-cache` with your favorite cache software
docker image, change the number of replicas to 4 and ensure the label matches
the one used in step 1 above `ha-cache=geo`.

****
`$ osc create -n <your-namespace-here> -f ./examples/geo-cache.json`
****

===== Step 3: Configure IP failover for the geo cache service
The final step is to configure the virtual IPs and failover for the nodes labelled in step 1 (with "ha-cache=geo"). Ensure the number of replicas matches the number of nodes that satisfy the constraint or label we used in step 1.  Specify the virtual IP address and the port number 9736 that IP failover should monitor on those instances.

****
`$ osadm ipfailover ha-geo-cache --replicas=4 --selector="ha-cache=geo" --virtual-ips=10.245.2.101-104 --watch-port=9736 --credentials="$OPENSHIFTCONFIG" --create`
****

==== Internals
The osadm ipfailover command ensures that an ipfailover sidecar pod runs on each of the nodes matching the constraints or label we used in step 1. This ipfailover sidecar pod uses VRRP (Virtual Router Reduncy Protocol) within keepalived to monitor that the service on the watched port is available and keepalived will automatically float the VIPs in event of the service not being available.

In our example, we can now use the VIPs 10.245.2.101 through 10.245.2.104 to send traffic to the geo-cache service. If a particular geo-cache instance is "unreachable" (as an example due to a node failure), keepalived will float the VIPs automatically float amongst group of nodes we labelled "ha-cache=geo" and the service would still be reachable via those virtual IP addresses.

== Examples

See what the IP failover configuration would look like if it is created:

****
`$ osadm ipfailover <options ...> -o json`
****


Create an IP failover configuration if it does not already exist:

****
`$ osadm ipfailover --virtual-ips="1.2.3.4-5,6.7.8.9,10.11.12.13-15" --create`
****


Create an IP failover configuration on a selection of nodes labelled "my-ha-service=har-reporter" (on 4 nodes with 7 virtual IPs monitoring a service listening on port 4242.

****
`$ osadm ipfailover harreporter --selector="my-ha-service=har-reporter" --virtual-ips="10.245.2.42,10.245.2.100-104,10.245.2.142,10.245.2.242" --watch-port=4242 --replicas=7 --create`
****
