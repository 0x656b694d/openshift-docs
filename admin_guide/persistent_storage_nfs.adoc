= Persistent Storage with NFS
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
This topic describes how to provision an OpenShift cluster with persistent storage using NFS.  Some familiarity with Kubernetes and NFS is assumed.

The Kubernetes
link:../dev_guide/volumes.html[Persistent Volume] framework allows
administrators to provision a cluster with persistent storage and gives users a way to request those resources without having
any knowledge of the underlying infrastructure.

IMPORTANT: High-availability of a storage in the infrastructure is left to the underlying storage provider.


== Provisioning

Storage must exist in the underlying infrastructure before it can be mounted as a volume in OpenShift.  All that's
required for NFS is a distinct list of servers/paths and the PersistentVolume API.

=== Enforcing disk quotas

Use disk partitions to enforce disk quotas and size constraints.  Each partition can be its own export.
Each export is 1 persistent volume.  Kubernetes enforces unique names for persistent volumes, but the uniqueness
of the NFS volume's server and path is up to the administrator.

Enforcing quotas in this way allows the end user to request persistent storage by a specific amount (e.g, 10Gi) and be
matched with a corresponding volume of equal or greater capacity.

=== Volume security

Users request storage with a *PersistentVolumeClaim*.  This claim only lives in the user's namespace and can only be referenced
by a pod within that same namespace.  Any attempt to access a persistent volume across a namespace will cause the pod to fail.

Each NFS volume must be mountable by all nodes in the cluster.


== Reclaiming resources

NFS implements the Kubernetes Recyclable plugin interface.  Automatic processes handle reclamation tasks based on policy
 set on each persistent volume.  By default, persistent volumes are set to ```Retain```.  NFS volumes which are set to ```Recycle``` are scrubbed
 (```"rm -rf"``` on the volume) after being released from their claim (i.e, the user's ```PersistentVolumeClaim``` bound to the volume was deleted).  Once recycled,
  the NFS volume can be bound to a new claim.


persistent-volume.json
====
----
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "pv0001"
  },
  "spec": {
    "capacity": {
        "storage": "5Gi"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/tmp",
        "server": "172.17.0.2"
    }
    "persistentVolumeReclaimPolicy": "Recycle"  // also "Retain"
  }
}
----
====


== Automation

In this topic, we covered how a cluster can be provisioned with persistent storage using NFS.  Disk partitions can be
used to enforce storage quotas, security is enforced by restricting volumes to the namespace that has a claim to them, and
reclamation of discarded resources can be configured for each persistent volume.

Many ways to script these tasks exist.  We've created an link:https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes[Ansible playbook] to help you get started.


== SELinux and NFS export settings

By default, SELinux will disallow a connection from a pod to a remote NFS server.  Enable this in SELinux on each node with the following command:

```
setsebool virt_use_nfs 1
```

Additionally, each exported volume on the NFS server itself must conform to the following:

* each export must be: ```/example_fs *(rw,no_root_squash)```
* each export is owned by nfsnobody: ```chmod nfsnobody:nfsnobody```
* each export must have the permissions: ```chmod 700```

Disabling SELinux entirely with ```setenforce 0``` would also work but is generally discouraged.
