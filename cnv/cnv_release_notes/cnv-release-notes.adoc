[id="cnv-release-notes"]
= {RN_BookName}
include::modules/cnv-document-attributes.adoc[]
:context: cnv-release-notes
toc::[]

== About {CNVProductName} {CNVVersion}

include::modules/cnv-what-you-can-do-with-cnv.adoc[leveloffset=+2]

=== {CNVProductNameStart} support

:FeatureName: {CNVProductNameStart}
include::modules/technology-preview.adoc[leveloffset=+2]

[id="cnv-2-3-new"]
== New and changed features

* {CNVProductNameStart} is certified in Microsoft's Windows Server
Virtualization Validation Program (SVVP) to run Windows Server workloads.
** SVVP Certification applies to Intel and AMD CPUs.
** SVVP Certificate Red Hat Enterprise Linux CoreOS 8 workers are named _Red Hat OpenShift
Container Platform 4 on RHEL CoreOS 8_.

* New templates are available that are compatible with Microsoft Windows 10.

[id="cnv-2-3-networking-new"]
=== Networking

* You can now use {CNVProductName} with either the xref:../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#learn-about-ovn-kubernetes[OVN-Kubernetes] or the xref:../../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] network provider.

* {CNVProductNameStart} uses link:https://nmstate.github.io/[`nmstate`] to report on and configure the state of the node network. You can now modify network policy configuration, such as creating or removing Linux bridges, bonds, and VLAN devices on all nodes, by applying a single configuration manifest to the cluster.

** For more information, see the xref:../../cnv/cnv_node_network/cnv-observing-node-network-state.adoc#cnv-about-nmstate_cnv-observing-node-network-state[node networking] chapter.

[id="cnv-2-3-storage-new"]
=== Storage

* You can now import, upload, and clone virtual machine disks into namespaces that are subject to CPU and memory resource restrictions.

* The `virtctl` tool can now monitor server-side upload post-processing asynchronously and more accurately reports the status of virtual machine disk uploads.

[id="cnv-2-3-web-new"]
=== Web console

* You can now view the *Paused* status of a virtual machine in the web console. The web console displays this status on the *Virtual Machines* dashboard and on the *Virtual Machine Details* page. +

[NOTE]
====
You can also view the *Paused* status of a virtual machine by using the `oc` CLI:
----
$ oc get vmi testvm -o=jsonpath='{.status.conditions[?(@.type=="Paused")].message}'
----
====

* If a virtual machine has a status of *Paused*, you can now xref:../../cnv/cnv_virtual_machines/cnv-controlling-vm-states.html#cnv-unpausing-vm-web_cnv-controlling-vm-states[unpause it from the
web console].

* You can view and manage virtual machine instances that are independent of virtual machines within the virtual machine lists.

* {CNVProductNameStart} enables you to configure a CD-ROM in the virtual machine wizard.  You can select the type of CD-ROM configuration from a drop-down list:  *Container*, *URL* or *Attach Disk*. You can also edit CD-ROM configurations in the *Virtual Machine Details* page.

* The boot order list is now available in the *Virtual Machine Details*. You can add items, remove items, and modify the boot order list.

* The virtual machine and virtual machine template wizards now validate CPU and memory sizes and disk bus requirements
for different operating systems. If you attempt to create a virtual machine or virtual machine template that does not meet the requirements for a particular operating system, the wizard raises a resource warning.

* You can now view and configure dedicated resources for virtual machines in
the *Virtual Machine Details* page. When you enable dedicated
resources, you ensure that your virtual machine's workload runs on CPUs that
are not used by other processes. This can improve your virtual machine's performance
and the accuracy of latency predictions.

[id="cnv-2-3-changes"]
== Notable technical changes

* You must deploy {CNVProductName} in a single namespace that is named
`openshift-cnv`. If it does not already exist, the `openshift-cnv` namespace is
now automatically created during deployment.

* To prevent errors, you can no longer rename the *CNV Operator Deployment* custom
resource that you create during {CNVProductName} deployment. If you try to create
a custom resource that is not named `kubevirt-hyperconverged`, which is the default
name, creation fails and an error message displays in the web console.

* To prevent unintentional data loss, you can no longer uninstall {CNVProductName} if your cluster has a virtual machine or DataVolume defined.
** You must manually delete all xref:../../cnv/cnv_virtual_machines/cnv-delete-vms.adoc#cnv-delete-vm-web_cnv-delete-vms[virtual machines] (VMs),
xref:../../cnv/cnv_virtual_machines/cnv-deleting-vmis.adoc#cnv-deleting-vmi_cnv-deleting-vmis[virtual machine instances] (VMIs),
and xref:../../cnv/cnv_virtual_machines/cnv_virtual_disks/cnv-deleting-datavolumes.adoc#cnv-deleting-dvs_cnv-deleting-datavolumes[DataVolumes] (DVs)
before uninstalling {CNVProductName}.
** If VM, VMI, or DV objects are present when you attempt to uninstall
{CNVProductName}, the uninstallation process does not complete until you remove
the remaining objects.
+
[NOTE]
====
To confirm that uninstallation is paused due to a pending object, view the
xref:../../cnv/cnv_logging_events_monitoring/cnv-events.adoc#cnv-viewing-vm-events-web_cnv-events[*Events*] tab.
====

[id="cnv-2-3-known-issues"]
== Known issues

* KubeMacPool is disabled in {CNVProductName} {CNVVersion}. This means that a secondary
interface of a Pod or virtual machine obtains a randomly generated MAC address rather
than a unique one from a pool. Although rare, randomly assigned MAC addresses can conflict.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1816971[*BZ#1816971*])

* Sometimes, when attempting to edit the subscription channel of the *{CNVProductNameStart} Operator* in the web console, clicking the
*Channel* button of the *Subscription Overview* results in a JavaScript error.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])

** As a workaround, trigger the upgrade process to {CNVProductName} {CNVVersion}
from the CLI by running the following `oc` patch command:
+
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.3 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.3` and enables automatic updates.

* In the virtual machine and virtual machine template wizards, *virtIO* is the default interface when you attach a CD-ROM. However, a *virtIO* CD-ROM does not pass virtual machine validation and cannot be created.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1817394[*BZ#1817394*])

** As a workaround, select *SATA* as the CD-ROM interface when you create virtual machines and virtual machine templates.

* The Containerized Data Importer (CDI) does not always use the `scratchSpaceStorageClass` setting in the CDIConfig object for importing and uploading operations.
Instead, the CDI uses the default storage class to allocate scratch space.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1828198[*BZ#1828198*])

** As a workaround, ensure you have defined a default storage class for your cluster. The following command can be used to apply the necessary annotation:
+
----
$ oc patch storageclass <STORAGE_CLASS_NAME> -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
----

* If you renamed the Operator deployment custom resource when you deployed
an earlier version of {CNVProductName}, you cannot upgrade directly to {CNVProductName} {CNVVersion}.
The custom resource must be named `kubevirt-hyperconverged`, which is the default
name. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822266[*BZ#1822266*])

** As a workaround, you can either:
*** Rename the existing custom resource to `kubevirt-hyperconverged`.
*** Create a new custom resource that is named the default `kubevirt-hyperconverged`.
Then, delete the custom resource that is not named `kubevirt-hyperconverged`.

* The {product-title} 4.4 web console includes *slirp* as an option when you add a NIC to a virtual machine, but *slirp* is not a valid NIC type. Do not select *slirp* when adding a NIC to a virtual machine.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1828744[*BZ#1828744*])

// For 2.3: Add new Known Issues above this line (so that we don't mix the new with the old/possibly no longer irrelevant ones)
// The following 7 bugs are from the 2.2 release notes but from their bugs they still seem to be relevant for 2.3:
* After migration, a virtual machine is assigned a new IP address. However, the
commands `oc get vmi` and `oc describe vmi` still generate output containing the
obsolete IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1686208[*BZ#1686208*])
+
** As a workaround, view the correct IP address by running the following command:
+
----
$ oc get pod -o wide
----

* Users without administrator privileges cannot add a network interface
to a project in an L2 network using the virtual machine wizard.
This issue is caused by missing permissions that allow users to load
network attachment definitions.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1743985[*BZ#1743985*])
+
** As a workaround, provide the user with permissions to load the network attachment
definitions.
+
. Define `ClusterRole` and `ClusterRoleBinding` objects to the YAML configuration
file, using the following examples:
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: cni-resources
rules:
- apiGroups: ["k8s.cni.cncf.io"]
 resources: ["*"]
 verbs: ["*"]
----
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: <role-binding-name>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cni-resources
subjects:
- kind: User
  name: <user to grant the role to>
  namespace: <namespace of the user>
----
+
. As a `cluster-admin` user, run the following command to create the `ClusterRole`
and `ClusterRoleBinding` objects you defined:
+
----
$ oc create -f <filename>.yaml
----

* Live migration fails when nodes have different CPU models. Even in cases where
nodes have the same physical CPU model, differences introduced by microcode
updates have the same effect. This is because the default settings trigger
host CPU passthrough behavior, which is incompatible with live migration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])
+
** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap,
as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support
live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this
value by running `oc describe node <node>` for all nodes and looking at the
`cpu-model-<name>` labels. Select the CPU model that is present on all of your
nodes.

* When attempting to create and launch a virtual machine using a Haswell CPU,
the launch of the virtual machine can fail due to incorrectly labeled nodes.
This is a change in behavior from previous versions of container-native
virtualization, where virtual machines could be successfully launched on Haswell hosts.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1781497[*BZ#1781497*])
+
As a workaround, select a different CPU model, if possible.

* The {CNVProductName} upgrade process occasionally fails due to an interruption
from the Operator Lifecycle Manager (OLM). This issue is caused by the limitations
associated with using a declarative API to track the state of {CNVProductName}
Operators. Enabling automatic updates during
xref:../cnv_install/installing-container-native-virtualization.adoc#cnv-subscribing-to-the-catalog_installing-container-native-virtualization[installation]
decreases the risk of encountering this issue.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1759612[*BZ#1759612*])

* {CNVProductNameStart} cannot reliably identify node drains that are triggered by
running either `oc adm drain` or `kubectl drain`. Do not run these commands on
the nodes of any clusters where {CNVProductName} is deployed. The nodes might not
drain if there are virtual machines running on top of them.
The current solution is to put nodes into maintenance.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1707427[*BZ#1707427*])

// Temporary comment: BZ1819700 PR (#20823) likely to conflict here because of updated version change.
* If you navigate to the *Subscription* tab on the *Operators* -> *Installed Operators*
page and click the current upgrade channel to edit it, there might be no visible results.
If this occurs, there are no visible errors.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1796410[*BZ#1796410*])
+
** As a workaround, trigger the upgrade process to {CNVProductName} {CNVVersion}
from the CLI by running the following `oc` patch command:
+
----
$ export TARGET_NAMESPACE=openshift-cnv CNV_CHANNEL=2.3 && oc patch -n "${TARGET_NAMESPACE}" $(oc get subscription -n ${TARGET_NAMESPACE} --no-headers -o name) --type='json' -p='[{"op": "replace", "path": "/spec/channel", "value":"'${CNV_CHANNEL}'"}, {"op": "replace", "path": "/spec/installPlanApproval", "value":"Automatic"}]'
----
+
This command points your subscription to upgrade channel `2.3` and enables automatic updates.
