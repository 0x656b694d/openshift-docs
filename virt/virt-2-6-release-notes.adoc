[id="virt-2-6-release-notes"]
= {RN_BookName}
include::modules/virt-document-attributes.adoc[]
include::modules/common-attributes.adoc[]
:context: virt-release-notes
toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]

[id="virt-guest-os"]
=== Supported guest operating systems
{VirtProductName} guests can use the following operating systems:

* Red Hat Enterprise Linux 6, 7, and 8.
* Microsoft Windows Server 2012 R2, 2016, and 2019.
* Microsoft Windows 10.

Other operating system templates shipped with {VirtProductName} are not supported.

[id="virt-2-6-new"]
== New and changed features

//CNV-7544: Microsoft's Windows Server Virtualization Validation Program (SVVP) applies to RHCOS workers

* OpenShift Virtualization is certified in Microsoftâ€™s Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red Hat OpenShift Container Platform 4 on RHEL CoreOS__.

** Intel and AMD CPUs.

//CNV-9723: VMs are now migrated when a node is drained
* Virtual machines that have the `LiveMigratable` condition set to `True` and the `spec.evictionStrategy` field set to `LiveMigrate` are now migrated when a node is drained in preparation for maintenance. You can xref:../virt/node_maintenance/virt-setting-node-maintenance.adoc#virt-setting-node-maintenance[set a node to maintenance mode in the CLI] by running the `oc adm drain` command. The `NodeMaintenance` custom resource definition is no longer supported.

//CNV-9521: VMs that use UEFI can now be booted by using secure boot

* You can now about xref:../virt/virtual_machines/advanced_vm_management/virt-efi-mode-for-vms.adoc#virt-efi-mode-for-vms[boot a virtual machine (VM) in Extensible Firmware Interface (EFI) mode].

+
[NOTE]
====
{VirtProductName} only supports a VM with Secure Boot when using EFI mode. If Secure Boot is not enabled, the VM crashes repeatedly. However, the VM might not support Secure Boot. Before you boot a VM, verify that it supports Secure Boot by checking the VM settings.
====


[id="virt-2-6-installation-new"]
=== Installation

//CNV-7294: The cluster admin can now influence the node placement of OpenShift Virtualization pods
// AND
//CNV-9340: The cluster admin can now influence the node placement of the HPP pods
* Cluster administrators can now xref:../virt/install/virt-specifying-nodes-for-virtualization-components.adoc#virt-specifying-nodes-for-virtualization-components[configure node placement rules] for {VirtProductName} components, including the xref:../virt/install/virt-specifying-nodes-for-virtualization-components.adoc#node-placement-hpp_virt-specifying-nodes-for-virtualization-components[hostpath provisioner].


[id="virt-2-6-networking-new"]
=== Networking

//CNV-9518: OpenShift Virtualization is now supported by Calico CNI


[id="virt-2-6-storage-new"]
=== Storage

//CNV-9454: On filesystem-based PVCs, CDI now accounts for FS overhead

//CNV-9455: Improvements to KubeVirt and CDI ensure that storage is allocated on the correct node

* When preparing local storage for a virtual machine disk image, a volume might be allocated from a different node than is required by the virtual machine. To prevent scheduling problems, the Containerized Data Importer (CDI) now integrates with the Kubernetes `WaitForFirstConsumer` binding mode to ensure that volumes are allocated from the correct node.

* The Containerized Data Importer (CDI) can now xref:../virt/virtual_machines/virtual_disks/virt-using-preallocation-for-datavolumes.adoc#virt-using-preallocation-for-datavolumes[preallocate disk space] to import and upload data and create blank data volumes at a faster speed.

//CNV-9456: When importing VM disk images, you can now control which multus network is used

[id="virt-2-6-web-new"]
=== Web console

//CNV-9312: The VM wizard has been completely redesigned, simplifying VM creation
* The virtual machine wizard has been redesigned to simplify the process of xref:../virt/virtual_machines/virt-create-vms.adoc#virt-create-vms[creating virtual machines] by using preconfigured xref:../virt/vm_templates/virt-creating-vm-template.adoc#virt-creating-vm-template[virtual machine templates].

* When you xref:../virt/vm_templates/virt-creating-vm-template.adoc#virt-creating-template-wizard-web_virt-creating-vm-template[create a virtual machine template with the interactive wizard], selecting the *Operating System* will automatically select the default *Flavor* and *Workload Type* for that operating system. Virtual machines created from a virtual machine template now have these details automatically selected.

[id="virt-2-6-changes"]
== Notable technical changes

//CNV-9339: When deploying from the GUI, the CR for the HCO is now created during subscription
* The procedure for xref:../virt/install/installing-virt-web.adoc#installing-virt-web[installing {VirtProductName} in the web console] has been streamlined. You can now create the `HyperConverged` custom resource immediately after installing the {VirtProductName} Operator by clicking *Create HyperConverged*.

//CNV-10153: BareMetalPlatform removed from HCO CR
* Previously, there was a `spec.BareMetalPlatform` field in the `HyperConverged` object. This field has been removed.

* The Containerized Data Importer (CDI) configuration parameters have moved from the `CDIConfig` object to the `CDI` object. All changes to the CDI configuration must now be made in the `spec.config` field of the `CDI` object.

[id="virt-2-6-known-issues"]
== Known issues

//New known issues for 2.6 (WIP)

* Some CDI operations are currently not xref:../virt/virtual_machines/virtual_disks/virt-using-preallocation-for-datavolumes.adoc#virt-using-preallocation-for-datavolumes[preallocated] when requested. These include:

** Creating blank block disks
** Importing VMWare disk images

//BZ-1855182 (https://bugzilla.redhat.com/show_bug.cgi?id=1855182): [Storage] Clone could not be continued after virtctl stop the vm if the clone dv have been created for more than 3 minutes

* The Containerized Data Importer (CDI) and KubeVirt depend on QEMU which does not support NFS version 3. Therefore, only NFS version 4 is supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1892445[*BZ#1892445*])

* The name of the Fedora PVC in the `openshift-virtualization-os-images` namespace is `fedora`, instead of `fedora32`. If you populated the `fedora32` PVC in {VirtProductname} 2.5 or earlier, the virtual machine does not appear in the web console and you cannot use it to clone another virtual machine. As a workaround, upload a Fedora image by naming the PVC `fedora`, instead of `fedora32`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1913352[*BZ#1913352*])

* When creating a HPP boot source, the data volume is `pending` with a `WaitForFirstConsumer` status if a user creates the boot source using any method except the *Upload local file (creates PVC)* option. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1929177[*BZ#1929177*])
+
As a workaround, in the *Storage* > *Persistent Volume Claims* web console screen, edit the YAML of the underlying PVC of the data volume to add the `cdi.kubevirt.io/storage.bind.immediate.requested: "true"` `annotation`:
+
[source,yaml]
----
metadata:
annotations: cdi.kubevirt.io/storage.bind.immediate.requested: "true"
----

* If you use a Fedora image as a boot source, it is no longer attached to a template if the PVC that you used to attach the boot source was previously provisioned. As a workaround, attach a new PVC with the name `fedora` to a template before using it to create virtual machines from boot sources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1907187[*BZ#1907187*]) (link:https://bugzilla.redhat.com/show_bug.cgi?id=1913352[*BZ#1913352*])

//The issues below are from the 2.5 release. See items above for new known issues for 2.6.
//The below issues have not yet been pruned/updated for 2.6.

* When a node fails in user-provisioned installations of {product-title} on bare metal deployments, the virtual machine does not automatically restart on another node. Automatic restart is supported only for installer-provisioned installations that have machine health checks enabled. Learn more about xref:/../virt/install/preparing-cluster-for-virt#preparing-cluster-for-virt[configuring your cluster for {VirtProductName}].

* If you xref:../virt/virtual_machines/importing_vms/virt-importing-vmware-vm.adoc#virt-creating-vddk-image_virt-importing-vmware-vm[add a VMware Virtual Disk Development Kit (VDDK) image] to the `openshift-cnv/v2v-vmware` config map by using the web console, a *Managed resource* error message displays. You can safely ignore this error. Save the config map by clicking *Save*. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1884538[*BZ#1884538*])

* When nodes are evicted, for example, when they are placed in maintenance mode during an {product-title} cluster upgrade, virtual machines are migrated twice instead of just once. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1888790[*BZ#1888790*])

//The issues below are from the 2.5 release. See items above for new known issues for 2.6.

* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. As a workaround, you can use a secondary network interface connected to your host or switch to the OpenShift SDN default CNI provider. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])

* Running virtual machines that cannot be live migrated might block an {product-title} cluster upgrade. This includes virtual machines that use hostpath-provisioner storage or SR-IOV network interfaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1858777[*BZ#1858777*])
+
As a workaround, you can reconfigure the virtual machines so that they can be powered off during a cluster upgrade. In the `spec` section of the virtual machine configuration file:
+
. Remove the `evictionStrategy: LiveMigrate` field. See xref:../virt/live_migration/virt-configuring-vmi-eviction-strategy.adoc#virt-configuring-vmi-eviction-strategy[Configuring virtual machine eviction strategy] for more information on how to configure eviction strategy.
. Set the `runStrategy` field to `Always`.

* Live migration fails when nodes have different CPU models. Even in cases where nodes have the same physical CPU model, differences introduced by microcode updates have the same effect. This is because the default settings trigger host CPU passthrough behavior, which is incompatible with live migration. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1760028[*BZ#1760028*])

** As a workaround, set the default CPU model in the `kubevirt-config` ConfigMap, as shown in the following example:
+
[NOTE]
====
You must make this change before starting the virtual machines that support live migration.
====
+
. Open the `kubevirt-config` ConfigMap for editing by running the following command:
+
[source,terminal]
----
$ oc edit configmap kubevirt-config -n openshift-cnv
----
+
. Edit the ConfigMap:
+
[source,yaml]
----
kind: ConfigMap
metadata:
  name: kubevirt-config
data:
  default-cpu-model: "<cpu-model>" <1>
----
<1> Replace `<cpu-model>` with the actual CPU model value. You can determine this value by running `oc describe node <node>` for all nodes and looking at the `cpu-model-<name>` labels. Select the CPU model that is present on all of your nodes.

* If you enter the wrong credentials for the RHV Manager while importing a RHV VM, the Manager might lock the admin user account because the `vm-import-operator` tries repeatedly to connect to the RHV API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1887140[*BZ#1887140*])
+
To unlock the account, log in to the Manager and enter the following command:
+
[source,terminal]
----
$ ovirt-aaa-jdbc-tool user unlock admin
----
