[id="ocp-4-5-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a more secure and scalable multi-tenant operating system for today's
enterprise-class applications, while delivering integrated application runtimes
and libraries. {product-title} enables organizations to meet security, privacy,
compliance, and governance requirements.

[id="ocp-4-5-about-this-release"]
== About this release

Red Hat {product-title}
(link:https://access.redhat.com/errata/RHBA-2020:2409[RHBA-2020:2409]) is now
available. This release uses link:https://v1-18.docs.kubernetes.io/docs/setup/release/notes/[Kubernetes 1.18] with CRI-O runtime. New features, changes, and known issues that pertain to
{product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.4.0 as the GA version and,
//instead, is releasing {product-title} 4.4.3 as the GA version.

{product-title} {product-version} clusters are available at
https://cloud.redhat.com/openshift. The {cloud-redhat-com}
application for {product-title} allows you to deploy OpenShift clusters to
either on-premise or cloud environments.

{product-title} {product-version} is supported on Red Hat Enterprise Linux 7.7 or
later, as well as {op-system-first} 4.5.

You must use {op-system} for the control plane, which are also known as master machines, and
can use either {op-system} or Red Hat Enterprise Linux 7.7 or later for
compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only Red Hat Enterprise Linux version 7.7 or later is supported for compute
machines, you must not upgrade the Red Hat Enterprise Linux compute machines to
version 8.
====

With the release of {product-title} 4.5, version 4.2 is now end of life. For
more information, see the
link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-5-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-5-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-5-installing-cluster-on-vsphere-using-installer-provisioned-infra"]
==== Installing a cluster on vSphere using installer-provisioned infrastructure

{product-title} 4.5 introduces support for installing a cluster on vSphere using
installer-provisioned infrastructure.

// For more information, see ../installing/installing_vsphere/installing-vsphere-installer-provisioned.adoc#installing-vsphere-installer-provisioned[Installing a cluster on vSphere]

[id="ocp-4-5-installing-cluster-on-gcp-using-user-provisioned-infra-and-shared-vpc"]
==== Installing a cluster on GCP using user-provisioned infrastructure and a shared VPC

{product-title} 4.5 introduces support for installing a cluster on Google Cloud
Platform (GCP) using user-provisioned infrastructure and a shared VPC.

// For more information, see ../installing//installing_gcp/installing-gcp-user-infra-vpc.adoc#installing-gcp-user-infra-vpc[Installing a cluster on GCP using Deployment Manager templates and a shared VPC].

[id="ocp-4-5-three-node-bare-metal-deployments"]
==== Three-node bare metal deployments

You can install and run three-node clusters in {product-title} with no workers.
This provides smaller, more resource efficient clusters for deployment,
development, and testing.

For more information, see
xref:../installing/installing_bare_metal/installing-bare-metal.adoc#installation-three-node-cluster_installing-bare-metal[Running a three-node cluster].

[id="ocp-4-5-restricted-network-cluster-upgrade-improvements"]
==== Restricted network cluster upgrade improvements

The Cluster Version Operator (CVO) can now verify the release images if the
image signature is available as a ConfigMap in the cluster during the upgrade
process for a restricted network cluster. This removes the need for using the
`--force` flag during upgrades in a restricted network environment.

This improved upgrade workflow is completed by running the enhanced
`oc adm release mirror` command. The following actions are performed:

* Pulls the image signature from the release during the mirroring process.
* Applies the signature ConfigMap directly to the connected cluster.

//For more information, see
//../updating/updating-restricted-network-cluster.adoc#updating-restricted-network-cluster[Updating a restricted network cluster].

[id="ocp-4-5-migrate-azure-private-dns-zones"]
==== Migrating Azure private DNS zones

There is now a new `openshift-install migrate` command available for migrating
Azure private DNS zones. If you installed an {product-title} version 4.2 or 4.3
cluster on Azure that uses installer-provisioned infrastructure, your cluster
might use a legacy private DNS zone. If it does, you must migrate it to the new
type of private DNS zone.

// For more information on this new command and migrating Azure private DNS
// zones, see ../installing/installing_azure/migrating-azure-dns-zones.adoc#migrating-azure-dns-zones[Migrating legacy Azure DNS zones].

[id="ocp-4-5-built-in-help-for-installconfig-supported-fields"]
==== Built-in help for `install-config.yaml` supported fields

There is a new `openshift-install explain` command available that lists all the
fields for supported `install-config.yaml` file versions including a short
description explaining each resource. It also provides details on which fields
are mandatory and specifies their default value. Using the `explain` command
reduces the need to continually look up configuration options when creating or
customizing the `install-config.yaml` file.

[id="ocp-4-5-encrypt-ebs-instance-volumes-with-kms-key"]
==== Encrypt EBS instance volumes with a KMS key

You can now define a KMS key to encrypt EBS instance volumes. This is useful if
you have explicit compliance and security guidelines when deploying to AWS. The
KMS key can be configured in the `install-config.yaml` file by setting the
optional `kmsKeyARN` field. For example:

[source,yaml]
----
apiVersion: v1
baseDomain: example.com
compute:
- architecture: amd64
  hyperthreading: Enabled
  name: worker
  platform:
    aws:
      rootVolume:
        kmsKeyARN: arn:aws:kms:us-east-2:563456982459:key/4f5265b4-16f7-xxxx-xxxx-xxxxxxxxxxxx
...
----

If no key is specified, the account's default KMS key for that particular region
is used.

[id="ocp-4-5-install-to-pre-existing-vpc-with-multiple-cidrs-on-aws"]
==== Install to pre-existing VPC with multiple CIDRs on AWS

You can now install {product-title} to a VPC with more than one CIDR on AWS.
This lets you select secondary CIDRs for the machine network. When the VPC is
provisioned by the installer, it does not create multiple CIDRs or configure the
routing between subnets. Installing to a pre-existing VPC with multiple CIDRs is
supported for both user-provisioned and installer-provisioned infrastructure
installation workflows.

[id="ocp-4-5-machine-api"]
=== Machine API

[id="ocp-4-5-aws-machinesets-support-spot-instances"]
==== AWS MachineSets support spot instances

AWS MachineSets now support spot instances. This lets you create a MachineSet
that deploys machines as spot instances so you can save costs compared to
on-demand instance prices. You can configure spot instances by adding the
following line under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  spotMarketOptions: {}
----

[id="ocp-4-5-autoscaling-the-minimum-number-of-machines"]
==== Autoscaling the minimum number of machines to 0

You can now set the minimum number of replicas for a MachineAutoscaler to `0`.
This allows the autoscaler to be more cost-effective by scaling between zero
machines and the machine count necessary based on the resources your workloads
require.

For more information, see the
xref:../machine_management/applying-autoscaling.adoc#machine-autoscaler-cr_applying-autoscaling[MachineAutoscaler resource definition].

[id="ocp-4-5-machinehealthcheck-with-empty-selector-monitors-all-machines"]
==== MachineHealthCheck with empty selector monitors all machines

A MachineHealthCheck resource that contains an empty `selector` field now
monitors all machines.

For more information on the `selector` field in the MachineHealthCheck resource,
see the
xref:../machine_management/deploying-machine-health-checks.html#machine-health-checks-resource_deploying-machine-health-checks[Sample MachineHealthCheck resource].

[id="ocp-4-5-nodes"]
=== Nodes

[id="ocp-4-5-descheduler-policy"]
==== New descheduler strategy is available (Technology Preview)

The descheduler now allows you to configure the `RemovePodsHavingTooManyRestarts` strategy. This strategy ensures that Pods that have been restarted too many times are removed from nodes.

See xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler-strategies_nodes-descheduler[Descheduler strategies]
for more information.

[id="ocp-4-5-node-pull-secrets"]
==== Node pull secrets

You can import and use images from any registry configured during or after the
cluster installation by sharing the node's pull secret credentials with the
`openshift-api`, `builder`, and `image-registry` Pods.

[id="ocp-4-5-vertical-pod-autoscaler-tp"]
==== Vertical Pod Autoscaler Operator (Technology Preview)

{product-title} {product-version} introduces the Vertical Pod Autoscaler Operator (VPA). The VPA reviews the historic and current CPU and memory resources for containers in Pods and can update the resource limits and requests based on the usage values it learns. You create individual custom resources (CR) to instruct the VPA to update all of the Pods associated with a workload object, such as a Deployment, Deployment Config, StatefulSet, Job, DaemonSet, ReplicaSet, or ReplicationController. The VPA helps you to understand the optimal CPU and memory usage for your Pods and can automatically maintain Pod resources through the Pod lifecycle.

[id="ocp-4-5-web-console"]
=== Web console

[id="ocp-4-5-new-attribute-filters"]
====  New Infrastructure Features filters for Operators in OperatorHub

You can now filter Operators by *Infrastructure Features* in OperatorHub. For
example, select *Disconnected* to see Operators that work in
disconnected environments.

[id="ocp-4-5-developer-perspective"]
==== Developer Perspective

You can now use the *Developer* perspective to:

* Make informed decisions on installing Helm Charts in the *Developer Catalog* using the description and docs for them.
* Uninstall, upgrade, and rollback Helm Releases.
* Create and delete dynamic Knative event sources.
* Deploy virtual machines, launch applications in them, or delete the virtual machines.
* Provide Git webhooks, Triggers, and  Workspaces, manage credentials of private git repositories, and troubleshoot using better logs for OpenShift Pipelines.
* Add health checks during or after application deployment.
* Navigate efficiently and pin frequently searched items.

[id="ocp-4-5-scale"]
=== Scale

[id="ocp-4-5-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around
xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[Cluster
maximums] for {product-title} {product-version} is now available.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title}
Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-5-networking"]
=== Networking

[id="ocp-4-5-mirgating-from-sdn-to-default-cni-tp"]
==== Migrating from the OpenShift SDN default CNI network provider (Technology Preview)

You can now migrate to the OVN-Kubernetes default Container Network Interface
(CNI) network provider from the OpenShift SDN default CNI network provider.

For more information, see
xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[Migrate from the OpenShift SDN default CNI network provider].

[id="ocp-4-5-ingress-enhancements"]
==== Ingress enhancements

There are two noteworthy Ingress enhancements introduced in {product-title} 4.5:

* You can enable access logs for the Ingress Controller.
//:../networking/ingress-operator.adoc#nw-configure-ingress-access-logging_configuring-ingress[enable access logs for the Ingress Controller].
* You can xref:../networking/ingress-operator.adoc#using-wildcard-routes_configuring-ingress[specify a wildcard route policy through the Ingress Controller].

[id="ocp-4-5-developer-experience"]
=== Developer experience

[id="ocp-4-5-oc-new-app-deployment-resources"]
==== oc new-app now produces Deployment resources

The `oc new-app` command now produces Deployment resources instead of
DeploymentConfig resources by default. If you prefer to create DeploymentConfig
resources, you can pass the `--as-deployment-config` flag when invoking `oc
new-app`. For more information, see
xref:../applications/deployments/what-deployments-are.adoc#what-deployments-are[Understanding Deployments and DeploymentConfigs].

[id="ocp-4-5-support-nodeaffinity-scheduler-in-image-registry-crd"]
==== Support node affinity scheduler in image registry CRD

The node affinity scheduler is now supported to ensure image registry
deployments complete even when an infrastructure node does not exist. The node
affinity scheduler must be manually configured.

See xref:../nodes/scheduling/nodes-scheduler-node-affinity.adoc#nodes-scheduler-node-affinity[Controlling Pod placement on nodes using node affinity rules]
for more information.

[id="ocp-4-5-disaster-recovery"]
=== Disaster recovery

[id="ocp-4-5-auto-cert-recovery"]
==== Automatic control plane certificate recovery

{product-title} can now automatically recover from expired control plane certificates. The exception is that you must manually approve pending `node-bootstrapper` certificate signing requests (CSRs) to recover kubelet certificates.

See xref:../backup_and_restore/disaster_recovery/scenario-3-expired-certs.adoc#dr-scenario-3-recovering-expired-certs_dr-recovering-expired-certs[Recovering from expired control plane certificates] for more information.

[id="ocp-4-5-storage"]
=== Storage

[id="ocp-4-5-persistent-storage-csi-ebs"]
==== Persistent storage using the AWS EBS CSI Driver Operator (Technology Preview)

You can now use the Container Storage Interface (CSI) to deploy the CSI driver you need for provisioning AWS Elastic Block Store (EBS) persistent storage. This Operator is in Technology Preview.
// Add the following text with link when https://github.com/openshift/openshift-docs/pull/22350 has merged.
//To enable it, see install instructions in link[AWS Elastic Block Store CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-manila"]
==== Persistent storage using the OpenStack Manila CSI Driver Operator

You can now use CSI to provision a PersistentVolume using the CSI driver for the OpenStack Manila shared file system service.
// Add the following text with link when https://github.com/openshift/openshift-docs/pull/22199 has merged.
//To enable it, see install instructions in link[OpenStack Manila CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-inline"]
==== Persistent storage using CSI inline ephemeral volumes (Technology Preview)

You can now use CSI to specify volumes directly in the Pod specification, rather than in a PersistentVolume. This feature is in Technology Preview and is available by default when using CSI drivers. For more information, see xref:../storage/container_storage_interface/ephemeral-storage-csi-inline.adoc#ephemeral-storage-csi-inline[CSI inline ephemeral volumes].

[id="ocp-4-5-persistent-storage-csi-cloning"]
==== Persistent storage using CSI volume cloning

Volume cloning using CSI, previously in Technology Preview, is now fully supported in OpenShift Container Platform 4.5. For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-cloning.adoc#persistent-storage-csi-cloning[CSI volume cloning].

[id="ocp-4-5-operators"]
=== Operators

[id="ocp-4-5-olm-v1-crd"]
==== v1 CRD support in Operator Lifecycle Manager

Operator Lifecycle Manager (OLM) now supports Operators using v1
CustomResourceDefinitions (CRDs) when loading Operators into catalogs and
deploying them on cluster. Previously, OLM only supported v1beta1 CRDs; OLM now
manages both v1 and v1beta1 CRDs in the same way.

To support this feature, OLM now enforces CRD upgrades are safer by ensuring
existing CRD storage versions are not missing in the upgraded CRD, avoiding
potential data loss.

[id="ocp-4-5-operator-api"]
==== Read-only Operator API (Technology Preview)

The new Operator API is now available as a Technology Preview feature in
read-only mode. Previously, installing Operators using Operator Lifecycle
Manager (OLM) required cluster administrators to be aware of multiple APIs,
including CatalogSources, Subscriptions, ClusterServiceVersions, and
InstallPlans. This single Operator API resource is a first step towards a more
simplified experience discovering and managing the lifecycle of Operators in a
{product-title} cluster.

Currently only available using the CLI and requiring a few manual steps to
enable, this feature previews interacting with Operators as a first-class API
object. Cluster administrators can discover previously installed Operators using
this API in read-only mode, for example using the `oc get operators` command.

To enable this Technology Preview feature:

.Procedure

. Disable xref:../architecture/architecture-installation.adoc#unmanaged-operators_architecture-installation[Cluster Version Operator (CVO) management] of the OLM:
+
----
$ oc patch clusterversion version \
    --type=merge -p \
    '{
       "spec":{
          "overrides":[
             {
                "kind":"Deployment",
                "name":"olm-operator",
                "namespace":"openshift-operator-lifecycle-manager",
                "unmanaged":true,
                "group":"apps/v1"
             }
          ]
       }
    }'
----

. Add the `OperatorLifecycleManagerV2=true` xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[FeatureGate]
to the OLM Operator.

.. Edit the OLM Operator's Deployment:
+
----
$ oc -n openshift-operator-lifecycle-manager \
    edit deployment olm-operator
----

.. Add the following flag to the Deployment's `args` section:
+
----
...
    spec:
      containers:
      - args:
...
        - --feature-gates
        - OperatorLifecycleManagerV2=true
----

.. Save your changes.

. Install an Operator using the normal OperatorHub method if you have not already;
this example uses an etcd Operator installed in the project `test-project`.

. Create a new Operator resource for the installed etcd Operator.

.. Save the following to a file:
+
.`etcd-test-op.yaml` file
----
apiVersion: operators.coreos.com/v2alpha1
kind: Operator
metadata:
  name: etcd-test
----

.. Create the resource:
+
----
$ oc create -f etcd-test-op.yaml
----

. To have the installed Operator opt in to the new API, apply the
`operators.coreos.com/etcd-test` label to the following objects related to your
Operator:
+
--
* Subscription
* InstallPlan
* ClusterServiceVersion
* Any CRDs owned by the Operator
--
+
[NOTE]
====
In a future release, these objects will be automatically labeled for any
Operators where the CSV was installed using a Subscription.
====
+
For example:
+
----
$ oc label sub etcd operators.coreos.com/etcd-test="" -n test-project
$ oc label ip install-6c5mr operators.coreos.com/etcd-test="" -n test-project
$ oc label csv etcdoperator.v0.9.4 operators.coreos.com/etcd-test="" -n test-project
$ oc label crd etcdclusters.etcd.database.coreos.com operators.coreos.com/etcd-test=""
$ oc label crd etcdbackups.etcd.database.coreos.com operators.coreos.com/etcd-test=""
$ oc label crd etcdrestores.etcd.database.coreos.com operators.coreos.com/etcd-test=""
----

. Verify your Operator has opted in to the new API.

.. List all `operators` resources:
+
----
$ oc get operators

NAME        AGE
etcd-test   17m
----

.. Inspect your Operator's details and note that the objects you labeled are
represented:
+
----
$ oc describe operators etcd-test

Name:         etcd-test
Namespace:
Labels:       <none>
Annotations:  <none>
API Version:  operators.coreos.com/v2alpha1
Kind:         Operator
Metadata:
  Creation Timestamp:  2020-07-02T05:51:17Z
  Generation:          1
  Resource Version:    37727
  Self Link:           /apis/operators.coreos.com/v2alpha1/operators/etcd-test
  UID:                 6a441a4d-75fe-4224-a611-7b6c83716909
Status:
  Components:
    Label Selector:
      Match Expressions:
        Key:       operators.coreos.com/etcd-test
        Operator:  Exists
    Refs:
      API Version:  apiextensions.k8s.io/v1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:40Z
        Message:               no conflicts found
        Reason:                NoConflicts
        Status:                True
        Type:                  NamesAccepted
        Last Transition Time:  2020-07-02T05:50:41Z
        Message:               the initial names have been accepted
        Reason:                InitialNamesAccepted
        Status:                True
        Type:                  Established
      Kind:                    CustomResourceDefinition
      Name:                    etcdclusters.etcd.database.coreos.com <1>
...
      API Version:             operators.coreos.com/v1alpha1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:39Z
        Message:               all available catalogsources are healthy
        Reason:                AllCatalogSourcesHealthy
        Status:                False
        Type:                  CatalogSourcesUnhealthy
      Kind:                    Subscription
      Name:                    etcd <2>
      Namespace:               test-project
...
      API Version:             operators.coreos.com/v1alpha1
      Conditions:
        Last Transition Time:  2020-07-02T05:50:43Z
        Last Update Time:      2020-07-02T05:50:43Z
        Status:                True
        Type:                  Installed
      Kind:                    InstallPlan
      Name:                    install-mhzm8 <3>
      Namespace:               test-project
...
      Kind:                    ClusterServiceVersion
      Name:                    etcdoperator.v0.9.4 <4>
      Namespace:               test-project
Events:                        <none>
----
<1> One of the CRDs.
<2> The Subscription.
<3> The InstallPlan.
<4> The CSV.

[id="ocp-4-5-notable-technical-changes"]
== Notable technical changes

{product-title} 4.5 introduces the following notable technical changes.

[discrete]
[id="ocp-4-5-osdk-v0-17-1"]
==== Operator SDK v0.17.2

{product-title} 4.5 supports Operator SDK v0.17.2, which introduces the
following notable technical changes:

* The `--crd-version` flag was added to the `new`, `add api`, `add crd`, and
`generate crds` commands so that users can opt-in to `v1` CRDs. The default setting
is `v1beta1`.

Ansible-based Operator enhancements include:

* Support for relative Ansible roles and playbooks paths in the Ansible-based Operator Watches files.
* Event statistics output to the Operator logs.

Helm-based Operator enhancements include:

* Support for Prometheus metrics.

[discrete]
[id="ocp-4-5-termination-grace-period-support"]
==== terminationGracePeriod parameter support

{product-title} now properly supports the `terminationGracePeriodSeconds`
parameter with the CRI-O container runtime.

[id="ocp-4-5-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to
be supported; however, it will be removed in a future release of this product
and is not recommended for new deployments. For the most recent list of major
functionality deprecated and removed within {product-title} {product-version},
refer to the table below. Additional details for more fine-grained functionality
that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.3 |OCP 4.4 |OCP 4.5

|Service Catalog
|DEP
|DEP
|REM

|Template Service Broker
|DEP
|DEP
|REM

|OpenShift Ansible Service Broker
|DEP
|REM
|REM

|OperatorSources
|DEP
|DEP
|DEP

|CatalogSourceConfigs
|DEP
|DEP
|REM

|Operator Framework's Package Manifest Format
|GA
|DEP
|DEP

|v1beta1 CRDs
|GA
|GA
|DEP

|====

[id="ocp-4-5-deprecated-features"]
=== Deprecated features

[id="ocp-4-5-deprecated-v1beta1-crds"]
==== v1beta1 CRDs

The `apiextensions.k8s.io/v1beta1` API version for CustomResourceDefinitions
(CRDs) is now deprecated. It will be removed in a future release of
{product-title}.

See xref:ocp-4-5-olm-v1-crd[v1 CRD support in Operator Lifecycle Manager] for related details.

[id="ocp-4-5-deprecation-of-operatorsources"]
==== OperatorSources and CatalogSourceConfigs block cluster upgrades

OperatorSources and CatalogSourceConfigs have been deprecated for several
{product-title} releases. Starting in {product-title} 4.4, if there are any
custom OperatorSources or CatalogSourceConfigs objects present on the cluster,
the `marketplace` cluster Operator sets an `Upgradeable=false` condition and
issues a *Warning* alert. This means that upgrades to {product-title} 4.5 are
blocked if the objects are still installed.

[NOTE]
====
Upgrades to {product-title} 4.4 z-stream releases are still permitted in this
state.
====

In {product-title} 4.5, OperatorSources are still deprecated and only exist for
the use of the default OperatorSources. CatalogSourceConfigs, however, are now
removed.

See the
link:https://docs.openshift.com/container-platform/4.4/release_notes/ocp-4-4-release-notes.html#ocp-4-4-marketplace-apis-deprecated[{product-title} 4.4 release notes]
for how to convert OperatorSources and CatalogSourceConfigs to using
CatalogSources directly, which clears the alert and enables cluster upgrades to
{product-title} 4.5.

[id="ocp-4-5-removed-features"]
=== Removed features

[id="ocp-4-5-oc-commands-flags-removed"]
==== OpenShift CLI commands and flags removed

The following `oc` commands and flags are affected:

* The `oc policy can-i` command was deprecated in {product-title} 3.9 and has
been removed. You must use `oc auth can-i` instead.

* The `--image` flag previously used for the `oc new-app` and `oc new-build`
commands was deprecated in {product-title} 3.2 and has been removed. You must
use the `--image-stream` flag with these commands instead.

* The `--list` flag previously used in the `oc set volumes` command was
deprecated in {product-title} 3.3 and has been removed. The `oc set volumes`
lists volumes without a flag.

* The `-t` flag previously used in the `oc process` command was deprecated in
{product-title} 3.11 and has been removed. You must use the `--template` flag
with this command instead.

* The `--output-version` flag previously used in the `oc process` command was
deprecated in {product-title} 3.11 and has been removed. This flag was already
ignored.

* The `-v` flag previously used in the `oc set deployment-hook` command was
deprecated in {product-title} 3.11 and has been removed. You must use the
`--volumes` flag with this command instead.

* The `-v` and `--verbose` flags previously used in the `oc status` command were
deprecated in {product-title} 3.11 and have been removed. You must use the
`--suggest` flag with this command instead.

[id="ocp-4-5-oc-run-pod"]
==== The `oc run` OpenShift CLI command now only creates Pods

The `oc run` command can now only be used to create Pods. Use the `oc create` command instead to create other resources.

[id="ocp-4-5-service-catalog-removed"]
==== Service Catalog, Template Service Broker, and their Operators

[IMPORTANT]
====
Service Catalog is not installed by default in {product-title} 4; however, it
now blocks upgrades to {product-title} 4.5 if installed.
====

Service Catalog, Template Service Broker, Ansible Service Broker, and their
associated Operators were deprecated starting in {product-title} 4.2. Ansible
Service Broker, including Ansible Service Broker Operator and related APIs and
APBs, were removed in {product-title} 4.4.

Service Catalog, Template Service Broker, and their associated Operators are now
removed in {product-title} 4.5, including the related
`.servicecatalog.k8s.io/v1beta1` API.

[NOTE]
====
Templates are still available in {product-title} 4.5, but they are no longer
handled by Template Service Broker. By default, the Samples Operator handles Red
Hat Enterprise Linux (RHEL)-based {product-title} ImageStreams and Templates.
See
xref:../openshift_images/configuring-samples-operator.adoc#configuring-samples-operator[Configuring the Samples Operator]
for details.
====

The `service-catalog-controller-manager` and `service-catalog-apiserver` cluster
Operators were set to `Upgradeable=false` in 4.4. This means that they block
cluster upgrades to the next minor version, 4.5 in this case, if they are still
installed at that time. Upgrades to z-stream releases such as 4.4.z, however,
are still permitted in this state.

If Service Catalog and Template Service Broker are enabled in 4.4, specifically
if their management state is set to `Managed`, the web console warns cluster
administrators that these features are still enabled. The following alerts can
be viewed from the *Monitoring* -> *Alerting* page on a 4.4 cluster and have a
*Warning* severity:

* `ServiceCatalogAPIServerEnabled`
* `ServiceCatalogControllerManagerEnabled`
* `TemplateServiceBrokerEnabled`

If they are still enabled on a 4.4 cluster, cluster administrators can see
link:https://docs.openshift.com/container-platform/4.4/applications/service_brokers/uninstalling-service-catalog.html#sb-uninstalling-service-catalog[Uninstalling Service Catalog] and link:https://docs.openshift.com/container-platform/4.4/applications/service_brokers/uninstalling-template-service-broker.html#sb-uninstalling-template-service-broker[Uninstalling Template Service Broker]
in the {product-title} 4.4 documentation to uninstall it, which permits cluster
upgrades to 4.5.

In 4.5, a pair of Jobs are created in a new `openshift-service-catalog-removed`
namespace to run during the cluster upgrade process. Their behavior depends on
the management state of Service Catalog:

* `Removed`: The Jobs remove the following Service Catalog items:
** Operators
** namespaces
** Custom Resources (CRs)
** ClusterRoles
** ClusterRoleBindings

* `Unmanaged`: The Jobs skip removal and do nothing.

* `Managed`: The Jobs report an error in logs. This state is unlikely to occur because
upgrades would have been blocked. The Jobs take no other actions.

The Jobs and `openshift-service-catalog-removed` namespace will be removed in a
future {product-title} release.

[NOTE]
====
As of {product-title} 4.5, all Red Hat-provided service brokers have been
removed. Any other broker installed by users is not removed by the upgrade
process. This is to avoid removing any services that might have been deployed
using the brokers. Users must remove these brokers manually.
====

[id="ocp-4-5-csc-removed"]
==== CatalogSourceConfigs removed

CatalogSourceConfigs are now removed. See
xref:ocp-4-5-deprecation-of-operatorsources[OperatorSources and CatalogSourceConfigs block cluster upgrades]
for more details.

[id="ocp-4-5-bug-fixes"]
== Bug fixes

* Previously, it was difficult to see the list of Pods or resources associated with a Knative service in the *Topology* view. With this bug fix, when you select the Knative service, the sidebar displays a list of Pods along with a link to see the logs. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1801752[*BZ#1801752*])
* When you edited an existing query using the PromQL editor in the *metrics* tab of the *Monitoring* view, the cursor moved to the end of the line. With this bug fix, the PromQL editor works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806114[*BZ#1806114*])
* For Knative images, in the *Add* > *From Git* option, the *Advanced Options* for *Routing* would not provide a prefetched container port option. Also, if you created the service without updating the default port value of `8080`, the revisions would not show. With this bug fix, the user can select from the available port options using the drop-down list or provide input if they want to use another port and the revisions are shown as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806552[*BZ#1806552*])
* Previously, a Knative service created using the CLI could not be edited using the console because the images could not be fetched. Now, if the associated ImageStreams are not found while editing, the value provided by the user for the container image in the YAML file is used. This allows the user to edit the service using the console, even if the service was created using the CLI. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1806994[*BZ#1806994*])
* In the *Topology* view, editing the image name in the external image registry for a Knative service did not create a new revision. With this bug fix, a new revision of the service is created when the name of the service is changed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1807868[*BZ#1807868*])
* When you used the *Add* > *Container Image* option, and then selected the *Image stream tag from internal registry* option, the *ImageStreams* drop-down list did not list the option to deploy images from the *OpenShift* namespace. However, you were able to access them through the CLI. With this bug fix, all users have access to images in the *OpenShift* namespace through the console and the CLI. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1822112[*BZ#1822112*])
* Previously, in the *Pipeline Builder*, when you edited a Pipeline that referenced a Task that did not exist, the entire screen would go white. This fix now displays an icon to indicate that an action is required and a drop-down list is displayed to easily update the Task reference. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1839883[*BZ#1839883*])
* In the *Pipelines Details* page, when you changed existing fields in the *Parameters* and the *Resources* tabs, the *Save* button was disabled even though the new changes were detected. The validation criteria has now been modified and the *Save* button is enabled to submit changes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1804852[*BZ#1804852*])
* In the *Add* > *From Git* option, the Pipeline templates provided by the OpenShift Pipelines Operator would fail when the *Deployment* or *Knative Services* resource options were selected. This bug fix adds support to use the resource type as well as the runtime to determine the Pipeline template, thus providing resource-specific Pipeline templates. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1796185[*BZ#1796185*])
* When a Pipeline was created using the *Pipeline Builder* and a Task parameter of the type array was used, the Pipeline did not start. With this bug fix, both array and string type parameters are supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1813707[*BZ#1813707*])
* In the *Topology* view, filtering nodes by application returned an error when the namespace had Operator-backed services. This bug fix adds the logic to filter out the Operator-backed service nodes based on the selected application group. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1810532[*BZ#1810532*])
* The *Developer Catalog* showed no catalog results until you selected the *Clear All Filters* option. With this bug fix, all catalog items are seen by default and you do not need to clear all filters. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1835548[*BZ#1835548*])

[id="ocp-4-5-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.3 |OCP 4.4 |OCP 4.5

|Precision Time Protocol (PTP)
|TP
|TP
|TP

|`oc` CLI Plug-ins
|TP
|TP
|TP

|experimental-qos-reserved
|TP
|TP
|TP

|Pod Unidler
|TP
|TP
|TP

|Ephemeral Storage Limit/Requests
|TP
|TP
|TP

|Descheduler
|-
|TP
|TP

|Podman
|TP
|TP
|TP

|Sharing Control of the PID Namespace
|TP
|TP
|GA

|OVN-Kubernetes Pod network provider
|TP
|TP
|TP

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|TP

|HPA for memory utilization
|TP
|TP
|TP

|Machine health checks
|TP
|GA
|GA

|Three-node bare metal deployments
|TP
|TP
|GA

|Helm CLI
|TP
|GA
|GA

|Service Binding
|TP
|TP
|TP

|Log forwarding
|TP
|TP
|TP

|User workload monitoring
|TP
|TP
|

|OpenShift Serverless
|TP
|GA
|GA

|Compute Node Topology Manager
|TP
|TP
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|-
|TP
|TP

|CSI volume cloning
|-
|TP
|GA

|CSI AWS EBS Driver Operator
|-
|-
|TP

|OpenStack Manila CSI Driver Operator
|-
|-
|GA

|CSI inline ephemeral volumes
|-
|-
|TP

|OpenShift Pipelines
|-
|TP
|TP

|Vertical Pod Autoscaler
|-
|-
|TP

|Operator API
|-
|-
|TP

|====

[id="ocp-4-5-known-issues"]
== Known issues

* When upgrading to a new {product-title} z-stream release, connectivity to the API server might be interrupted as nodes are upgraded, causing API requests to fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1845411[*BZ#1845411*])

* When upgrading to a new {product-title} z-stream release, connectivity to routers might be interrupted as router pods are updated. For the duration of the upgrade, some applications might not be consistently reachable. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1809665[*BZ#1809665*])

* Because the `ImageContentSourcePolicy` for image registry pull-through is not
yet supported, the deployment Pod cannot mirror images by using a digest ID if
the imagestream has the pull-through policy enabled. In this case, an
`ImagePullBackOff` error displays.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1787112[*BZ#1787112*])

* The Che Workspace Operator was updated to use the DevWorkspace custom resource instead of the Workspace custom resource. However, the OpenShift web terminal continues to use the Workspace custom resource. Because of this, the OpenShift web terminal fails to work with the latest version of the Che Workspace Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1846969[*BZ#1846969*])

* A `basic-user` is unable to view the *Dashboard* and *Metrics* tabs in the *Monitoring* view of the *Developer* perspective. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1846409[*BZ#1846409*])

* In the *Topology* view, when you right-click a Knative service, the *Edit Application Grouping* option is displayed twice in the context menu.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=1849107[*BZ#1849107*])


[id="ocp-4-5-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.5 are released
as asynchronous errata through the Red Hat Network. All {product-title} 4.5
errata is https://access.redhat.com/downloads/content/290/[available on the Red
Hat Customer Portal]. See the
https://access.redhat.com/support/policy/updates/openshift[{product-title}
Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account
settings for Red Hat Subscription Management (RHSM). When errata notifications
are enabled, users are notified via email whenever new errata relevant to their
registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming
{product-title} entitlements for {product-title} errata notification
emails to generate.
====

This section will continue to be updated over time to provide notes on
enhancements and bug fixes for future asynchronous errata releases of
{product-title} 4.5. Versioned asynchronous releases, for example with the form
{product-title} 4.5.z, will be detailed in subsections. In addition, releases
in which the errata text cannot fit in the space provided by the advisory will
be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on
xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster]
properly.
====

[[rhba-2020-2409]]
=== RHBA-2020:2409 - {product-title} 4.5 Image release and bug fix advisory

Issued: 2020-07-09

{product-title} release 4.5 is now available. The list of container images and
bug fixes includes in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2020:2409[RHBA-2020:2409] advisory.
The RPM packages included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2020:2408[RHBA-2020:2408] advisory.

Space precluded documenting all of the container images for this release in the
advisory. See the following article for notes on the container images in this
release:

link:https://access.redhat.com/solutions/5184131[{product-title} 4.5.0 container image list]
