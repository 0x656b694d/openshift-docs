[id="ocp-4-6-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid
cloud application platform for deploying both new and existing applications on
secure, scalable resources with minimal configuration and management overhead.
{product-title} supports a wide selection of programming languages and
frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on Red Hat Enterprise Linux and Kubernetes, {product-title}
provides a more secure and scalable multi-tenant operating system for today's
enterprise-class applications, while delivering integrated application runtimes
and libraries. {product-title} enables organizations to meet security, privacy,
compliance, and governance requirements.

[id="ocp-4-6-about-this-release"]
== About this release

// TODO: Update this link once there is a 1.19-specific URL for the Kubernetes 1.19 release notes (requested in https://github.com/kubernetes/website/issues/23855)
Red Hat {product-title}
(link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234]) is now
available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.19] with CRI-O runtime. New features, changes, and known issues that pertain to
{product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.5.0 as the GA version and,
//instead, is releasing {product-title} 4.5.1 as the GA version.

{product-title} {product-version} clusters are available at
https://cloud.redhat.com/openshift. The {cloud-redhat-com}
application for {product-title} allows you to deploy OpenShift clusters to
either on-premise or cloud environments.

{product-title} {product-version} is supported on Red Hat Enterprise Linux 7.7 or
later, as well as {op-system-first} 4.6.

You must use {op-system} for the control plane, which are also known as master machines, and
can use either {op-system} or Red Hat Enterprise Linux 7.7 or later for
compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only Red Hat Enterprise Linux version 7.7 or later is supported for compute
machines, you must not upgrade the Red Hat Enterprise Linux compute machines to
version 8.
====

With the release of {product-title} 4.6, version 4.3 is now end of life. For
more information, see the
link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-6-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-6-rhcos"]
=== {op-system-first}

[id="ocp-4-6-live-env-with-coreos-installer"]
==== {op-system} PXE and ISO now live environment

The PXE media and ISO available for {op-system} are now a fully live
environment. Unlike the previous dedicated PXE media and ISO used for
installation, the {product-title} live environment can be configured with
Ignition and contains all the same packages as the main {op-system}, such as
podman. This allows arbitrary scripting of pre- or post-installation workflows. For
example, you could run `coreos-installer` and then make an HTTP request to
signal success to a provisioning server. PXE boots use the normal
`ignition.config.url`. The ISO can be configured with Ignition by using the
following command:

[source,terminal]
----
$ coreos-installer iso ignition embed
----

[id="ocp-4-6-coreos-installer-rewritten"]
==== `coreos-installer` has been rewritten

The `coreos-installer` is now rewritten to support more features including:

* Modifying the kernel arguments of the installed system.
* Fetching Ignition configs.
* Preserving previously existing partitions.
* Configuring Ignition for the new live ISO using the `coreos-installer iso ignition` command.

[id="ocp-4-6-ignition-spect-updated-v3"]
==== Ignition Spec updated to v3

{op-system} now uses Ignition spec v3 as the only supported spec
version of Ignition. This allows for more complex disk configuration support
in the future.

The change should be mostly transparent for those using installer-provisioned
infrastructure. For user-provisioned infrastructure installations, you must
adapt any custom Ignition configurations to use Ignition spec 3. The
`openshift-install` program now generates Ignition spec 3.

If you are creating Machine Configs for day 1 or day 2 operations that use
Ignition snippets, they should be created using Ignition spec v3. However, the
Machine Config Operator (MCO) still supports Ignition spec v2.

[id="ocp-4-6-extensions-supported-for-rhcos-mco"]
==== Extensions now supported for {op-system} and MCO 

{op-system} and the MCO now support the following extensions to the default
{op-system} installation.

* `kernel-devel`
* `usbguard`

[id="ocp-4-6-4kn-disk-support"]
==== 4Kn Disks now supported

{op-system} now supports installing to disks that use 4K sector sizes.

[id="ocp-4-6-4k-var-partition-support"]
==== `/var` partitions now supported

{op-system} now supports `/var` being a separate partition, as well as any other
subdirectory of `/var`.

[id="ocp-4-6-static-ip-config-with-ova"]
==== Static IP configuration for vSphere using OVA

You can now override default Dynamic Host Configuration Protocol (DHCP)
networking by setting the `guestinfo.afterburn.initrd.network-kargs` property
before booting a VM from an OVA in vSphere:

[source,terminal]
----
$ govc vm.change -vm "<vm_name>" -e "guestinfo.afterburn.initrd.network-kargs=<static_ip_config>"
----

This lowers the barrier for automatic {op-system-first} deployment in
environments without DHCP. This enhancement allows for higher-level automation
to provision an {op-system} OVA in environments with static networking.

For more information, see
link:https://bugzilla.redhat.com/show_bug.cgi?id=1785122[*BZ1785122*].

[id="ocp-4-6-security"]
=== Security

[id="ocp-4-6-oauth-token-inactivity-timeout"]
==== Configure OAuth token inactivity timeout

You can now configure OAuth tokens to expire after a certain amount of time that they have been inactive. By default, there is no token inactivity timeout set. You can configure the timeout for the internal OAuth server and for OAuth clients.

See xref:../authentication/configuring-internal-oauth.adoc#oauth-token-inactivity-timeout_configuring-internal-oauth[Configuring token inactivity timeout for the internal OAuth server] and xref:../authentication/configuring-oauth-clients.adoc#oauth-token-inactivity-timeout_configuring-oauth-clients[Configuring token inactivity timeout for an OAuth client] for more information.

[id="ocp-4-6-oauth-token-storage"]
==== Secure OAuth token storage format

OAuth access token and OAuth authorize token object names are now stored as non-sensitive object names.

Previously, secret information was used as the OAuth access token and OAuth authorize token object names. When etcd is encrypted, only the value is encrypted, so this sensitive information was not encrypted.

[IMPORTANT]
====
If you are upgrading your cluster to {product-title} 4.6, old tokens from {product-title} 4.5 will still have the secret information exposed in the object name. By default, the expiration for tokens is 24 hours, but this setting can be changed by administrators. Sensitive data can still be exposed until all old tokens have either expired or have been deleted by an administrator.
====

[id="ocp-4-6-machine-api"]
=== Machine API

[id="ocp-4-6-azure-machinesets-support-spot-vms"]
==== MachineSets running on Azure support Spot VMs

MachineSets running on Azure now support Spot VMs. You can create a MachineSet
that deploys machines as Spot VMs to save on costs compared to
standard VM prices.
//For more information, see ../../machine_management/creating_machinesets/creating-machineset-azure.adoc#machineset-non-guaranteed-instance_creating-machineset-azure[MachineSets that deploy machines as Spot VMs].

Configure Spot VMs by adding `spotVMOptions` under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  value:
    spotVMOptions: {}
----

[id="ocp-4-6-gcp-machinesets-support-preemptible-vm-instances"]
==== MachineSets running on GCP support preemptible VM instances

MachineSets running on GCP now support preemptible VM instances. You can create a MachineSet
that deploys machines as preemptible VM instances to save on costs compared to
normal instance prices.
//For more information, see ../../machine_management/creating_machinesets/creating-machineset-gcp.adoc#machineset-non-guaranteed-instance_creating-machineset-gcp[MachineSets that deploy machines as preemptible VM instances].

Configure preemptible VM instances by adding `preemptible` under the `providerSpec` field in the MachineSet YAML file:

[source,yaml]
----
providerSpec:
  value:
    preemptible: true
----

[id="ocp-4-6-web-console"]
=== Web console

[id="ocp-4-6-web-console-improved-upgrade-experience"]
==== Improved upgrade experience in the web console

* Administrators are now better informed about the differences between the upgrade
channels by helpful text and links in the web console.
* A link to the list of bug fixes and enhancements is now included for each minor
or patch release.
* There is now a visual representation of the different upgrade paths.
* Alerts now inform administrators when new patch releases, new minor releases,
and news channels become available.

[id="ocp-4-6-web-console-improved-operator-installation-workflow"]
==== Improved Operator installation workflow with OperatorHub

When administrators install Operators with OperatorHub, they now get immediate
feedback to ensure that the Operator is installing properly.

[id="ocp-4-6-web-console-improved-operand-details-view"]
==== Improved operand details view

You can now see the schema grouping of `specDescriptor` fields and the status of
your Operands on the operand's details view, so that you can easily see the
status and configure the `spec` of the operand instance.

[id="ocp-4-6-web-console-view-operators-related-objects"]
==== View related objects for cluster Operators

Previously, when viewing a cluster Operator, it was not clear what resources the
Operator was associated with. When troubleshooting a cluster Operator, it could
be challenging to locate the logs for all the resources that the Operator
managed, which might be needed for troubleshooting. Now, with {product-title}
{product-version}, you can expose a list of related objects of a cluster
Operator and easily review one of the related objects' details or YAML code for
troubleshooting.

[id="ocp-4-6-web-console-warning-messages-when-editing-managed-resources"]
==== Warning messages when editing managed resources

Some resources are managed, such an Operator managed by a deployment, route,
service, or ConfigMap. Users are discouraged from editing these resources.
Instead, users should edit the custom resources for the Operator and its
operand, and expect the Operator to update its related resources. With this
update:

* A *Managed by* label now appears below the resource name with a clickable
resource link for the managing resource.
* When the resource is modified or deleted, a message appears warning the user
that their changes might be reverted.

[id="ocp-4-6-web-console-specDescriptor-supports-CRD-instance"]
====  The `k8sResourcePrefix` specDescriptor supports CRD instance

Operator authors, maintainers, and providers can now specify the
`k8sResourcePrefix` specDescriptor with `Group/Version/Kind` for assigning a CRD
resource type besides Kubernetes core API.

For more information, see
link:https://github.com/openshift/console/blob/master/frontend/packages/operator-lifecycle-manager/src/components/descriptors/reference/reference.md[OLM
Descriptor Reference].

[id="ocp-4-6-web-console-column-management"]
==== Column management on resources page

A *Manage columns* icon image:manage-columns.png[title="Manage Columns icon"] is
now added to some resources pages, for example the Pods page. When you click on
the icon, default column names are listed with check boxes on the left side of
the modal and additional column names are listed on the right. Deselecting a
check box will remove that column from the table view. Selecting a check box
will add that column to the table view. A maximum combination of nine columns
from both sides of the modal are available for display at one time. Clicking
*Save* will save the changes that you make. Clicking *Restore Default Columns*
will restore the default settings of the columns.

[id="ocp-4-6-scale"]
=== Scale

[id="ocp-4-6-scale-cluster-maximums"]
==== Cluster maximums

Updated guidance around
xref:../scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc#planning-your-environment-according-to-object-maximums[cluster
maximums] for {product-title} {product-version} is now available.

Use the link:https://access.redhat.com/labs/ocplimitscalculator/[{product-title}
Limit Calculator] to estimate cluster limits for your environment.

[id="ocp-4-6-scale-real-time-profile-added-to-node-tuning-operator"]
==== Real-time profile added to the Node Tuning Operator

Partial Tuned real-time profile support became available in {product-title} 4.4.
Now, the real-time profiles are fully compatible with what the real-time
profiles do in Tuned on {op-system-base-full}.

[id="ocp-4-6-networking"]
=== Networking

[id="ocp-4-6-ovn-kubernetes-ga"]
==== OVN-Kubernetes default Pod network provider GA

The OVN-Kubernetes default Pod network provider is now GA. For more information, including details on feature parity with OpenShift SDN, refer to xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[About the OVN-Kubernetes default Container Network Interface (CNI) network provider].

For this release, OpenShift SDN remains the default Pod network provider.
[id="ocp-4-6-expand-node-service-port-range"]
==== Expand node service port range

The node service port range is expandable beyond the default range of `30000-32767`. You can use this expanded range in your `Service` objects. For more information, refer to xref:../networking/configuring-node-port-service-range.adoc#configuring-node-port-service-range[Configuring the node port service range].

[id="ocp-4-6-sr-iov-infiniband-devices"]
==== SR-IOV Network Operator InfiniBand device support

The Single Root I/O Virtualization (SR-IOV) Network Operator now supports InfiniBand (IB) network devices.
// For more information on configuring an IB network device for your cluster, refer to xref::../networking/hardware_networks/...[This link].

[id="ocp-4-6-pod-network-connectivity-checks"]
==== Pod network connectivity checks

Operators can now configure `PodNetworkConnectivityCheck` resources to check each network connection from the Pods that are managed by the Operator. This allows you to more easily identify and troubleshoot issues with important network connections in your cluster.

This resource keeps track of the latest reachable condition, the last 10 successes, the last 10 failures, and details about detected outages. The results are also logged and events are created when outages are detected and resolved.

By default, the following network connections are checked:

* Between the Kubernetes API server and:
** the OpenShift API server service
** each OpenShift API server endpoint
** each etcd endpoint
** the internal API load balancer
** the external API load balancer

* Between the OpenShift API server and:
** the Kubernetes API server service
** each Kubernetes API server endpoint
** each etcd endpoint
** the internal API load balancer
** the external API load balancer

[id="ocp-4-6-storage"]
=== Storage

[id="ocp-4-6-csi-driver-cso"]
==== CSI drivers now managed by the Cluster Storage Operator
The Container Storage Interface (CSI) Driver Operators and drivers for xref:../storage/container_storage_interface/persistent-storage-csi-ebs.adoc#persistent-storage-csi-ebs[AWS Elastic Block Store (EBS)], xref:../storage/container_storage_interface/persistent-storage-csi-ovirt.adoc#persistent-storage-csi-ovirt[Red Hat Virtualization (oVirt)], and xref:../storage/container_storage_interface/persistent-storage-csi-manila.adoc#persistent-storage-csi-manila[OpenStack Manila shared file system service] are now managed by the Cluster Storage Operator in {product-title}.

For AWS EBS and oVirt, this feature installs the CSI Driver Operator and driver in the `openshift-cluster-csi-drivers` namespace by default. For Manila, the CSI Driver Operator is installed in `openshift-cluster-csi-drivers` and the driver is installed in the `openshift-manila-csi-driver` namespace.

[IMPORTANT]
====
If you installed a CSI Driver Operator and driver on an {product-title} 4.5 cluster:

* The AWS EBS CSI Driver Operator and driver must be uninstalled before you update to a newer version of {product-title}.
* The OpenStack Manila CSI Driver Operator is no longer available in Operator Lifecycle Manager (OLM). It has been automatically converted by the Cluster Version Operator.
====

[id="ocp-4-6-lso-automation"]
==== Automatic device discovery and provisioning with the Local Storage Operator
The Local Storage Operator now has the ability to:

* Automatically discover a list of available disks in a cluster. You can select a list of nodes, or all nodes, for auto-discovery to be continuously applied to.
* Automatically provision local persistent volumes from attached devices. Appropriate devices are filtered and persistent volumes are provisioned based on the filtered devices.

[id="ocp-4-6-operators"]
=== Operators

[id="ocp-4-6-metering-operator"]
==== Configuring a retention period of metering Reports

You can now set a retention period on a metering Report. The metering Report custom resource
has a new `expiration` field. If the `expiration` duration value is set on a Report,
and no other Reports or ReportQueries depend on the expiring Report, the Metering Operator
removes the Report from your cluster at the end of its retention period. For more information,
see metering Reports xref:../metering/reports/metering-about-reports.adoc#metering-expiration_metering-about-reports[expiration].

[id="ocp-4-6-notable-technical-changes"]
== Notable technical changes

{product-title} 4.6 introduces the following notable technical changes.

[discrete]
[id="ocp-4-6-ovn-k8s-default-cni-np-uses-ovs-on-cluster-nodes"]
==== OVN-Kubernetes default CNI network provider now uses OVS installed on cluster nodes

The OVN-Kubernetes default Container Network Interface (CNI) network provider now uses the Open vSwitch (OVS) version installed on the cluster nodes. Previously, OVS ran in a container on each node, managed by a DaemonSet. Using the host OVS eliminates any possible downtime from upgrading the containerized version of OVS.

[discrete]
[id="ocp-4-6-warnings-when-using-deprecated-apis"]
==== Warnings when using deprecated APIs

Warnings are now visible in `client-go` and `oc` on every invocation against a
deprecated API. Calling a deprecated API returns a warning message containing
the target Kubernetes removal release and replacement API, if applicable.

For example:

[source,terminal]
----
warnings.go:67] batch/v1beta1 CronJob is deprecated in v1.22+, unavailable in v1.25+
----

This is new functionality included with Kubernetes 1.19.

[id="ocp-4-6-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to
be supported; however, it will be removed in a future release of this product
and is not recommended for new deployments. For the most recent list of major
functionality deprecated and removed within {product-title} {product-version},
refer to the table below. Additional details for more fine-grained functionality
that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.4 |OCP 4.5 |OCP 4.6

|Service Catalog
|DEP
|REM
|REM

|Template Service Broker
|DEP
|REM
|REM

|OperatorSources
|DEP
|DEP
|REM

|CatalogSourceConfigs
|DEP
|REM
|REM

|Operator Framework's Package Manifest Format
|DEP
|DEP
|DEP

|v1beta1 CRDs
|GA
|DEP
|DEP

|====

[id="ocp-4-6-deprecated-features"]
=== Deprecated features

[id="ocp-4-6-removed-features"]
=== Removed features

[id="ocp-4-6-bug-fixes"]
== Bug fixes 

*Web console (Developer perspective)*

* Previously, when you tried to delete a Knative application through the *Topology* view, a false positive error about a non-existing *Knative route* was reported. This issue is now fixed and the error is no longer displayed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1866214[BZ#1866214])

* Previously, the Developer Console did not allow images from insecure registries to be imported. This bug fix adds a checkbox that allows users to use the insecure registries in the *Deploy image* form. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1826740[BZ#1826740])

* When a user selected the *From Catalog* option to create an application, the *Developer Catalog* displayed a blank page instead of a list of templates to create an application. This was caused when the 1.18.0 Jaeger Operator was installed. This issue has now been fixed and the templates are displayed as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1845279[BZ#1845279])

* When deleting a parallel task in a Pipeline through the *Pipeline Builder* in the Developer Console, the interface was rearranging the tasks connected to the parallel task incorrectly, creating orphan tasks. With this fix, the tasks connected to the deleted parallel task are reconnected with the original Pipeline. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1856155[BZ#1856155])

* The web console was crashing with a JavaScript exception when the user cancelled the creation of a Pipeline through the web console with a side panel opened at the same time. This was fixed by improving the internal state handling. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1856267[BZ#1856267])

* A user with the required permissions was unable to retrieve and deploy an image from another project. The required RoleBindings have now been created to fix this issue. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1843222[BZ#1843222])

* When you tried to deploy an application from a Git repository with the *Import from Git* function, the Developer Console reported a false positive error `Git repository is not reachable` for private repositories reachable by the cluster. This was fixed by adding information on making the private repository available to the cluster in the error message. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1877739[BZ#1877739])

* When a Go application was created through the Developer Console, a route to the application was not created. This was caused by a bug in `build-tools` and incorrectly configured ports. The issue has been fixed by picking either the user-provided port or the default port 8080 as the target port. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1874817[BZ#1874817])

* When you created an application with the *Import from Git* function, a subsequent change of the application's Git repository from the web console was not possible. This was caused by changing the application name in subsequent editing of the Git repository URL. This was fixed by making the application name read-only when editing the application Git repository URL. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1873095[BZ#1873095])

* Previously, a user without administrative or project listing privileges could not see the metrics of any projects. This bug fix removes the checks for user privileges when accessing the cluster metrics. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1842875[BZ#1842875])

* Users with the `@` character in their user names, like `user@example.com`, could not start a Pipeline from the Developer Console. This was caused by a limitation in Kubernetes labels. The issue was fixed by moving the "Started by" metadata from a Kubernetes label to a Kubernetes annotation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1868653[BZ#1868653])


[id="ocp-4-6-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These
experimental features are not intended for production use. Note the
following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview
Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.4 |OCP 4.5 |OCP 4.6

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI Plug-ins
|TP
|TP
|

|experimental-qos-reserved
|TP
|TP
|

|Pod Unidler
|TP
|GA
|GA

|Ephemeral Storage Limit/Requests
|TP
|TP
|

|Descheduler
|TP
|TP
|

|Podman
|TP
|TP
|

|Sharing Control of the PID Namespace
|TP
|GA
|GA

|OVN-Kubernetes Pod network provider
|TP
|TP
|

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|

|HPA for memory utilization
|TP
|TP
|

|Three-node bare metal deployments
|TP
|GA
|GA

|Service Binding
|TP
|TP
|

|Log forwarding
|TP
|GA
|GA

|User workload monitoring
|TP
|TP
|

|Compute Node Topology Manager
|TP
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|

|External provisioner for AWS EFS
|TP
|TP
|

|CSI volume snapshots
|TP
|TP
|

|CSI volume cloning
|TP
|TP
|GA

|CSI AWS EBS Driver Operator
|-
|TP
|TP

|OpenStack Manila CSI Driver Operator
|-
|GA
|GA

|Red Hat Virtualization (oVirt) CSI Driver Operator
|-
|-
|GA

|CSI inline ephemeral volumes
|-
|TP
|TP

|OpenShift Pipelines
|TP
|TP
|TP

|Vertical Pod Autoscaler
|-
|TP
|

|Operator API
|-
|TP
|

|====

[id="ocp-4-6-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.7 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.6, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP 403 errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

* Running the `operator-sdk new` or `operator-sdk create api` commands without the
`--helm-chart` flag builds a Helm-based Operator that uses the default
boilerplate Nginx chart. While this example chart works correctly on upstream
Kubernetes, it fails to deploy successfully on {product-title}.
+
To work around this issue, use the `--helm-chart` flag to provide a Helm chart
that deploys successfully on {product-title}. For example:
+
[source,terminal]
----
$ operator-sdk new <operator_name> --type=helm \
  --helm-chart=<repo>/<name>
----
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1874754[*BZ#1874754*])

* When installing {product-title} on bare metal nodes with the Redfish Virtual
Media feature, a failure occurs when the Baseboard Management Controller (BMC)
attempts to load the virtual media image from the provisioning network. This
happens if the BMC is not using the provisioning network, or its network does
not have routing set up to the provisioning network. As a workaround, when using
virtual media, the provisioning network must be turned off, or the BMCs must be
routed to the provisioning network as a prerequisite.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1872787[*BZ#1872787*])

[id="ocp-4-6-asynchronous-errata-updates"]
== Asynchronous errata updates

[[rhba-2020-1234]]
=== RHBA-2020:1234 - {product-title} 4.6 image release and bug fix advisory

Issued: 2020-xx-xx

{product-title} release 4.6 is now available. The list of container images and
bug fixes includes in the update are documented in the
link:https://access.redhat.com/errata/RHBA-2020:xxxx[RHBA-2020:xxxx] advisory.
The RPM packages included in the update are provided by the
link:https://access.redhat.com/errata/RHBA-2020:xxxx[RHBA-2020:xxxx] advisory.

Space precluded documenting all of the container images for this release in the
advisory. See the following article for notes on the container images in this
release:

link:https://access.redhat.com/solutions/[{product-title} 4.6.0 container image list]
