[[install-config-downgrade]]
= Downgrading OpenShift
{product-author}
{product-version}
:icons: font
:experimental:
:toc: macro
:toc-title:
:prewrap!:
:description: Manual steps to revert {product-title} to a previous version following an upgrade.
:keywords: yum

toc::[]

== Overview

Following an {product-title}
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade],
it may be desirable in extreme cases to downgrade your cluster to a previous
version. The following sections outline the required steps for each system in a
cluster to perform such a downgrade for the {product-title} 3.4 to 3.3 downgrade
path.

[WARNING]
====
These steps are currently only supported for
xref:../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM-based
installations] of {product-title} and assumes downtime of the entire cluster.
====

[[downgrade-verifying-backups]]
== Verifying Backups

The Ansible playbook used during the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade process] should have created
a backup of the *_master-config.yaml_* file and the etcd data directory. Ensure
these exist on your masters and etcd members:

----
/etc/origin/master/master-config.yaml.<timestamp>
/var/lib/origin/etcd-backup-<timestamp>
----

Also, back up the *_node-config.yaml_* file on each node (including masters,
which have the node component on them) with a timestamp:

----
/etc/origin/node/node-config.yaml.<timestamp>
----

If you use a separate etcd cluster instead of a single embedded etcd instance, the
backup is likely created on all etcd members, though only one is required for
the recovery process. You can run a separate etcd instance that is co-located
with your master nodes.

The RPM downgrade process in a later step should create *_.rpmsave_* backups of
the following files, but it may be a good idea to keep a separate copy
regardless:

----
/etc/sysconfig/atomic-openshift-master
/etc/etcd/etcd.conf <1>
----
<1> Only required if using a separate etcd cluster.


[[downgrade-shutting-down-the-cluster]]
== Shutting Down the Cluster

On all masters, nodes, and etcd members, if you use a separate etcd cluster
that runs on different nodes, ensure the relevant services are stopped.

On the master in a single master cluster:

----
# systemctl stop atomic-openshift-master
----

On each master in a multi-master cluster:

----
# systemctl stop atomic-openshift-master-api
# systemctl stop atomic-openshift-master-controllers
----


On all master and node hosts:

----
# systemctl stop atomic-openshift-node
----

On any etcd hosts for a separate etcd cluster:

----
# systemctl stop etcd
----

[[downgrade-removing-rpms]]
== Removing RPMs

. The **-excluder* packages add entries to the exclude directive in the host’s
*_/etc/yum.conf_* file when installed. Run the following command on each host to
remove the `atomic-openshift-*` and `docker` packages from the exclude list:
+
----
# atomic-openshift-excluder unexclude
# atomic-openshift-docker-excluder unexclude
----

. On all masters, nodes, and etcd members, if you use a separate etcd cluster
that runs on different nodes, remove the following packages:
+
----
# yum remove atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node \
    atomic-openshift-excluder \
    atomic-openshift-docker-excluder
----

. If you use a separate etcd cluster, also remove the *etcd* package:
+
----
# yum remove etcd
----
+
If using the embedded etcd, leave the *etcd* package installed. It is required
for running the `etcdctl` command to issue operations in later steps.

[[downgrade-docker]]
== Downgrading Docker

{product-title} 3.3 requires Docker 1.10.3.

Downgrade to Docker 1.10.3. on each host using the following steps:

. Remove all local containers and images on the host. Any pods backed by a
replication controller will be recreated.
+
[WARNING]
====
The following commands are destructive and should be used with caution.
====
+
Delete all containers:
+
----
# docker rm $(docker ps -a -q)
----
+
Delete all images:
+
----
# docker rmi $(docker images -q)
----

. Use `yum swap` (instead of `yum downgrade`) to install Docker 1.10.3:
+
----
# yum swap docker-* docker-*1.10.3
# sed -i 's/--storage-opt dm.use_deferred_deletion=true//' /etc/sysconfig/docker-storage
# systemctl restart docker
----

. You should now have Docker 1.10.3 installed and running on the host. Verify
with the following:
+
----
# docker version
Client:
 Version:      1.10.3-el7
 API version:  1.20
 Package Version: docker-1.10.3.el7.x86_64
[...]

# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2017-06-21 15:44:20 EDT; 30min ago
[...]
----

[[downgrade-reinstalling-rpms]]
== Reinstalling RPMs

. Disable the {product-title} 3.4 repositories, and re-enable the 3.3
repositories:
+
----
# subscription-manager repos \
    --disable=rhel-7-server-ose-3.4-rpms \
    --enable=rhel-7-server-ose-3.3-rpms
----

. On each master, install the following packages:
+
----
# yum install atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node \
    atomic-openshift-excluder \
    atomic-openshift-docker-excluder
----

. On each node, install the following packages:
+
----
# yum install atomic-openshift \
    atomic-openshift-node \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node \
    atomic-openshift-excluder \
    atomic-openshift-docker-excluder
----

. If you use a separate etcd cluster, install the following package on each etcd
member:
+
----
# yum install etcd
----

[[downgrade-restore-etcd]]
== Restoring etcd

See
xref:../admin_guide/backup_restore.adoc#admin-guide-backup-and-restore[Backup
and Restore].

[[downgrade-bringing-openshift-services-back-online]]
== Bringing {product-title} Services Back Online

See xref:../admin_guide/backup_restore.adoc#bringing-openshift-services-back-online[Backup
and Restore].

[[verifying-the-downgrade]]
== Verifying the Downgrade

. To verify the downgrade, first check that all nodes are marked as *Ready*:
+
----
# oc get nodes
NAME                        STATUS                     AGE
master.example.com          Ready,SchedulingDisabled   165d
node1.example.com           Ready                      165d
node2.example.com           Ready                      165d
----

. Then, verify that you are running the expected versions of the *docker-registry*
and *router* images, if deployed:
+
----
ifdef::openshift-enterprise[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift3/ose-docker-registry:v3.3.1.38-2",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift3/ose-haproxy-router:v3.3.1.38-2",
----

. You can use the
xref:../admin_guide/diagnostics_tool.adoc#admin-guide-diagnostics-tool[diagnostics
tool] on the master to look for common issues and provide suggestions:
+
----
# oc adm diagnostics
...
[Note] Summary of diagnostics execution:
[Note] Completed with no errors or warnings seen.
----
