// Module included in the following assemblies:
//CNF-1483 (4.8)
// * scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc


[id="reducing-nic-queues-using-the-performance-addon-operator_{context}"]
= Reducing NIC queues using the Performance Addon Operator

The Performance Addon Operator allows you to adjust the Network Interface Card  (NIC) queue count for each network device by configuring the performance profile. Device network queues allow packets to be distributed among different physical queues and each queue gets a separate thread for packet processing.

In real-time or low latency systems all the unnecessary interrupt request lines (IRQs) that are pinned to the isolated CPUs must be moved to reserved or housekeeping CPUs.

In deployments with applications that require system, {product-title} networking or in mixed deployments with Data Plane Development Kit (DPDK) workloads, multiple queues are needed to achieve good throughput and the number of NIC queues should either be adapted or remain unchanged. For example, to achieve low latency the number of NIC queues for DPDK based workloads should be reduced to just the number of reserved or housekeeping CPUs.

Too many queues are created by default for each CPU and these do not fit into the interrupt tables for house keeping CPUs when tuning for low latency. Reducing the number of queues makes proper tuning possible. Smaller number of queues means smaller number of interrups which then fit in the IRQ table.

[id="adjusting-nic-queues-with-the-performance-profile_{context}"]
== Adjusting the NIC queues with the performance profile

The performance profile lets you adjust the queue count for each network device.

Supported network devices are:

* Non virtual network devices

* Network devices that support multiple queues (channels)

Unsupported network devices are:

* Pure software network interfaces

* Block devices

* Intel DPDK virtual functions

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.
* Install the OpenShift CLI (`oc`).


.Procedure

. Log in to the OpenShift Container Platform cluster running the Performance Addon Operator as a user with `cluster-admin` privileges.

. Create a performance profile that is appropriate for your hardware and topology. For more information see the Tuning nodes for low latency with the performance profile section. Alternatively, edit an existing performance profile using the following command:
+
[source,terminal]
----
$ oc edit performanceprofile <your_profile_name>
----

. Populate the `spec` field profile with the `net` object. The object list can contain two fields:

* `userLevelNetworking` is a required field that specifies a boolean flag which, if `true` sets the queue count to the reserved CPU count for all supported devices. The default is `false`.
* `Devices` is an optional field which specifies a list of devices that will have the queues set to the reserved CPU count. If the device list is empty the configuration applies to all network devices. The configuration is as follows:
** `interfaceName`: Name supports shell-style wildcards which can be positive or negative.
*** Example wildcard syntax is as follows: `<string> .*`
*** Negative rules are prefixed with an exclamation mark. To apply the net queue changes to all devices other than the excluded list, use  `!<device>`; for example, `!eno1`.
* `vendorID`: Network device vendor ID represented as a 16 bit hexmadecimal number with a 0x prefix.
* `deviceID`: Network device ID (model) represented as a 16 bit hexmadecimal number with a 0x prefix.
+
[NOTE]
====
When a `deviceID` is specified, the `vendorID` must also be defined. A device that matches all of the device identifiers specified in a device entry `interfaceName`, `vendorID` or a pair of `vendorID` plus `deviceID` representing a network device qualifies as a network device that will have its net queues count set to the reserved CPU count.

When two or more devices are specified, the net queues count is set to any net device that matches one of them.
====
+
An example performance profile configuration is shown below:
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
  kind: PerformanceProfile
  metadata:
    name: manual
  spec:
    cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54

    net:
    - userLevelNetworking: true         <1>
    #!

    more examples:

      net:
        userLevelNetworking: true
        devices:
            -interfaceName: “eth0”
            -interfaceName: “eth1”      <2>
            -vendorID: “0x1af4”
             deviceID: “0x1000”

      net:
       userLevelNetworking: true        <3>
       devices:
        -interfaceName: “eth*”

      net:
       userLevelNetworking: true        <4>
       devices:
        -interfaceName: “!eno1”

     net:
       userLevelNetworking: true        <5>
       devices:
        -interfaceName: “eth0”
         vendorID: “0x1af4”
         deviceID: “0x1000”

    #!

      nodeSelector:
          node-role.kubernetes.io/worker-cnf: ""

----
Set the queue count to the reserved CPU count for:
<1>  All devices.
<2>  All devices that match any of the defined device identifiers.
<3>  All devices starting with the interface name `eth`.
<4>  All devices with an interface named anything other than `eno1`.
<5>  All devices that have an interface name `eth0`, `vendorID` 0x1af4, and `deviceID` 0x1000.

. Apply the performance profile.
+
[source,terminal]
----
$  oc apply -f <your_profile_name>.yaml
----

[id="verify-queue-status_{context}"]
== Verify the queue status

In this section, a number of examples illustrate different performance profiles and how to verify the changes are applied.

.Example 1

In this example, the net queue count is set to the reserved CPU count (2) for _all_ supported devices.

The relevant section from the performance profile is:

[source,yaml]
----
apiVersion: performance.openshift.io/v2
metadata:
 name: performance
spec:
  kind: PerformanceProfile
  spec:
   cpu:
    reserved: 0-1  #total = 2
    Isolated: 2-8
   net:
    userLevelNetworking: true
    [...]
----

Display the status of the queues associated with a device using the command:
[source,terminal]
----
$ ethtool -l <device>
----
[NOTE]
====
Run this command on the node where the performance profile was applied.
====

Before the profile is applied the queue status is:

[source,terminal]
----
# ethtool -l ens4
Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   4
----
After the profile is applied the queue status is:

[source,terminal]
----
# ethtool -l ens4
Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <1>
----
<1> The combined channel shows the total count of reserved CPUs for _all_ supported devices is 2. This matches what is configured in the performance profile.

.Example 2

In this example, the net queue count is set to the reserved CPU count (2) for _all_ supported network devices with a specific `vendorID`.

The relevant section from the performance profile is:

[source,yaml]
----
apiVersion: performance.openshift.io/v2
metadata:
  name: performance
spec:
    kind: PerformanceProfile
    spec:
     cpu:
      reserved: 0-1  #total = 2
      Isolated: 2-8
     net:
      userLevelNetworking: true
      devices:
      - vendorID = 0x1af4
    [...]
----

Display the status of the queues associated with a device using the command:
[source,terminal]
----
$ ethtool -l <device>
----
[NOTE]
====
Run this command on the node where the performance profile was applied.
====

Verify the queue status after the profile is applied:

[source,terminal]
----
# ethtool -l ens4
Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <1>
----

<1> The total count of reserved CPUs for all supported devices with `vendorID=0x1af4` is 2.
For example, if there is another network device `ens2` with `vendorID=0x1af4` it will also have total net queues of 2. This matches what is configured in the performance profile.

.Example 3

In this example, the net queue count is set to the reserved CPU count (2) for _all_ supported network devices that match any of the defined device identifiers.

The command `udevadm info` provides a detailed report on a device. In this example the devices are:

[source,terminal]
----
# udevadm info -p /sys/class/net/ens4
...
E: ID_MODEL_ID=0x1000
E: ID_VENDOR_ID=0x1af4
E: INTERFACE=ens4
…
----

[source,terminal]
----
# udevadm info -p /sys/class/net/eth0
...
E: ID_MODEL_ID=0x1002
E: ID_VENDOR_ID=0x1001
E: INTERFACE=eth0
...
----

Set the net queues to 2 for a device with `interfaceName` equal to `eth0` and any devices that have a `vendorID=0x1af4` with the following performance profile:

[source,yaml]
----
apiVersion: performance.openshift.io/v2
metadata:
    name: performance
spec:
    kind: PerformanceProfile
    spec:
     cpu:
      reserved: 0-1  #total = 2
      Isolated: 2-8
     net:
      userLevelNetworking: true
      devices:
      - interfaceName = eth0
      - vendorID = 0x1af4
    [...]
----

Verify the queue status after the profile is applied:

[source,terminal]
----
# ethtool -l ens4
Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <1>
----

<1> The total count of reserved CPUs for all supported devices with `vendorID=0x1af4` is set to 2.
For example, if there is another network device `ens2` with `vendorID=0x1af4`, it will also have the total net queues set to 2. Similarly, a device with `interfaceName` equal to `eth0` will have total net queues set to 2.

[id="logging-associated-with-adjusting-nic-queues_{context}"]
== Logging associated with adjusting NIC queues

Log messages detailing the assigned devices are recorded in the respective tuned daemon logs. The following messages may be recorded to `/var/log/tuned/tuned.log`:

* An INFO message is recorded detailing the successfully assigned devices:
+
[source, terminal]
----
INFO tuned.plugins.base: instance net_test (net): assigning devices ens1, ens2, ens3
----
* A WARNING message is recorded if none of the devices can be assigned:
+
[source, terminal]
----
WARNING  tuned.plugins.base: instance net_test: no matching devices available
----
