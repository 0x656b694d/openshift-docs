= OpenShift SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

OpenShift uses a software-defined networking (SDN) approach to provide a unified
cluster network that enables communication between pods across the
OpenShift cluster. This cluster network is established and maintained by the
ifdef::openshift-origin[]
https://github.com/openshift/openshift-sdn[OpenShift SDN],
endif::[]
ifdef::openshift-enterprise[]
OpenShift SDN,
endif::[]
which configures an overlay network using Open vSwitch (OVS).

ifdef::openshift-origin[]
OpenShift SDN provides two SDN plug-ins for configuring the network:

* The *ovssubnet* plug-in is the original plug-in which provides a "flat" pod
network where every pod can communicate with every other pod and service.
* The *multitenant* plug-in provides OpenShift project level isolation for
pods and services. Each project receives a unique Virtual Network ID (VNID)
that identifies traffic from pods assigned to the project. Pods from different
projects cannot send packets to or receive packets from pods and services of
a different project.
+
However, projects which receive VNID 0 are more privileged in
that they are allowed to communicate with all other pods, and all other pods can
communicate with them. In OpenShift clusters, the *default* project has
VNID 0. This facilitates certain services like the load balancer, etc. to
communicate with all other pods in the cluster and vice versa.
endif::[]

ifdef::openshift-enterprise[]
OpenShift SDN includes the *ovssubnet* SDN plug-in for configuring the network,
which provides a "flat" pod network where every pod can communicate with every
other pod and service.
endif::[]

Following is a detailed discussion of the design and operation of OpenShift SDN,
which may be useful for troubleshooting.

[[sdn-design-on-masters]]

== Design on Masters

On an OpenShift master, OpenShift SDN maintains a registry of nodes, stored in
*etcd*. When the system administrator registers a node, OpenShift SDN allocates
an unused subnet from the cluster network and stores this subnet in the
registry. When a node is deleted, OpenShift SDN deletes the subnet from the
registry and considers the subnet available to be allocated again.

In the default configuration, the cluster network is the *10.1.0.0/16* class B
network, and nodes are allocated */24* subnets (i.e., *10.1.0.0/24*,
*10.1.1.0/24*, *10.1.2.0/24*, and so on). This means that the cluster network
has 256 subnets available to assign to nodes, and a given node is allocated 254
addresses that it can assign to the containers running on it. The size and
address range of the cluster network are configurable, as is the host subnet
size.

Note that OpenShift SDN on a master does not configure the local (master) host
to have access to any cluster network. Consequently, a master host does not have
access to pods via the cluster network, unless it is also running as a
node.

ifdef::openshift-origin[]
When using the *multitenant* plug-in, the OpenShift SDN master also watches for
the creation and deletion of projects, and assigns VXLAN VNIDs to them, which
will be used later by the nodes to isolate traffic correctly.
endif::[]

=== Master Network Configuration

Cluster admin will have the option to control these pod network settings on the master:

[source,yaml]
----
...
networkConfig:
  clusterNetworkCIDR: 10.1.0.0/16    # Cluster network for node IP allocation
  hostSubnetLength: 8                # Number of bits for Pod IP allocation within a node
  networkPluginName: ""              # redhat/openshift-ovs-subnet for flat SDN
ifdef::openshift-origin[]
	                             # redhat/openshift-ovs-multitenant for multitenant SDN
endif::[]
  serviceNetworkCIDR: 172.30.0.0/16  # Service IP allocation for the cluster
...
----

[[sdn-design-on-nodes]]

== Design on Nodes

On a node, OpenShift SDN first registers the local host with the SDN master in
the aforementioned registry so that the master allocates a subnet to the node.

Next, OpenShift SDN creates and configures six network devices:

* *br0*, the OVS bridge device that pod containers will be attached to. OpenShift
SDN also configures a set of non-subnet-specific flow rules on this bridge.
ifdef::openshift-origin[]
The *multitenant* plug-in does this immediately.
endif::[]
The *ovssubnet* plug-in waits to do so until the SDN master announces the
creation of the new node subnet.
* *lbr0*, a Linux bridge device, which is configured as Docker's bridge and
given the cluster subnet gateway address (eg, 10.1.x.1/24).
* *tun0*, an OVS internal port (port 2 on *br0*). This _also_ gets assigned the
cluster subnet gateway address, and is used for external network
access. OpenShift SDN configures *netfilter* and routing rules to enable access
from the cluster subnet to the external network via NAT.
* *vlinuxbr* and *vovsbr*, two Linux peer virtual Ethernet interfaces.
*vlinuxbr* is added to *lbr0* and *vovsbr* is added to *br0*
ifdef::openshift-origin[]
(port 9 in *ovssubnet* plug-in,  port 3 in *multitenant* plug-in)
endif::[]
ifndef::openshift-origin[]
(port 9)
endif::[]
, to provide connectivity for containers created directly with Docker outside of OpenShift.
* *vxlan0*, the OVS VXLAN device (port 1 on *br0*), which provides access to
containers on remote nodes.

Each time a pod is started on the host, OpenShift SDN:

. moves the host side of the pod's veth interface pair from the *lbr0* bridge
(where Docker placed it when starting the container) to the OVS bridge *br0*.
. adds OpenFlow rules to the OVS database to route traffic addressed to the new
pod to the correct OVS port.
ifdef::openshift-origin[]
. in the case of the *multitenant* plug-in, adds OpenFlow rules to tag traffic
coming from the pod with the pod's VNID, and to allow traffic into the pod if
the traffic's VNID matches the pod's VNID (or is the privileged VNID 0).
(Non-matching traffic is filtered out by a generic rule.)
endif::[]

The pod is allocated an IP address in the cluster subnet by Docker itself
because Docker is told to use the *lbr0* bridge, which OpenShift SDN has
assigned the cluster gateway address (eg. 10.1.x.1/24). Note that the *tun0* is
also assigned the cluster gateway IP address because it is the default gateway
for all traffic destined for external networks, but these two interfaces do not
conflict because the *lbr0* interface is only used for IPAM and no OpenShift SDN
pods are connected to it.

OpenShift SDN nodes also watch for subnet updates from the SDN master. When a new
subnet is added, the node adds OpenFlow rules on *br0* so that packets with a
destination IP address the remote subnet go to *vxlan0* (port 1 on *br0*) and thus
out onto the network.
ifdef::openshift-origin[]
The *ovssubnet* plug-in sends all packets across the VXLAN with VNID 0, but the
*multitenant* plug-in uses the appropriate VNID for the source container.
endif::[]

=== Node Network Configuration

Cluster admin will have the option to control these pod network settings on the node:

[source,yaml]
----
...
networkConfig:
  mtu: 1450              # Maximum transmission unit for the pod overlay network
  networkPluginName: ""  # redhat/openshift-ovs-subnet for flat SDN
ifdef::openshift-origin[]
	                 # redhat/openshift-ovs-multitenant for multitenant SDN
endif::[]
...
----

=== Packet Flow

Suppose we have two containers A and B where the peer virtual Ethernet device
for container A's *eth0* is named *vethA* and the peer for container B's *eth0*
is named *vethB*.

[NOTE]
====
If Docker's use of peer virtual Ethernet devices is not already familiar to you,
review https://docs.docker.com/articles/networking[Docker's advanced networking
documentation].
====

Now suppose first that container A is on the local host and container B is also
on the local host. Then the flow of packets from container A to container B is
as follows:

*_eth0_* _(in A's netns) -> *vethA* -> *br0* -> *vethB* -> *eth0* (in B's netns)_

Next, suppose instead that container A is on the local host and container B is
on a remote host on the cluster network. Then the flow of packets from container
A to container B is as follows:

*_eth0_* _(in A's netns) -> *vethA* -> *br0* -> *vxlan0* ->
network footnote:[After this point, device names refer to devices on container
B's host.] -> *vxlan0* -> *br0* -> *vethB* -> *eth0* (in B's netns)_

Finally, if container A connects to an external host, the traffic looks like:

*_eth0_* _(in A's netns) -> *vethA* -> *br0* -> *tun0* -> (NAT) -> *eth0* (physical device) -> Internet_

Almost all packet delivery decisions are performed with OpenFlow rules in the
OVS bridge *br0*, which simplifies the plug-in network architecture and provides
flexible routing.
ifdef::openshift-origin[]
In the case of the *multitenant* plug-in, this also provides enforceable network
isolation.
endif::[]

ifdef::openshift-origin[]
[[multitenant-plugin]]

=== Network Isolation With the multitenant Plug-in

When using the *multitenant* plug-in, when a packet exits a pod assigned to a
non-default project, the OVS bridge *br0* tags that packet with the project's
assigned VNID. If the packet is directed to another IP address in the node's
cluster subnet, the OVS bridge only allows the packet to be delivered to the
destination pod if the VNIDs match.

If a packet is received from another node via the VXLAN tunnel, the Tunnel ID
is used as the VNID, and the OVS bridge only allows the packet to be delivered
to a local pod if the tunnel ID matches the destination pod's VNID.

Packets destined for other cluster subnets are tagged with their VNID and
delivered to the VXLAN tunnel with a tunnel destination address of the node
owning the cluster subnet.

As described before, VNID 0 is privileged in that traffic with any VNID is
allowed to enter any pod assigned VNID 0, and traffic with VNID 0 is allowed to
enter any pod. Only the *default* OpenShift project is assigned VNID 0; all
other projects are assigned unique, isolation-enabled VNIDs. Admin can optionally
control the pod network for the project with
link:../../admin_guide/pod_network.html[oadm pod-network] CLI command.
endif::[]

=== External Access to the Cluster Network

If a host that is external to OpenShift requires access to the cluster network,
you have two options:

. Configure the host as an OpenShift node but mark it
link:../../admin_guide/manage_nodes.html#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring link:../../admin_guide/routing_from_edge_lb.html[routing from an
edge load-balancer to containers within OpenShift SDN].

ifdef::openshift-origin[]
=== Migration between ovssubnet and multitenant plug-ins
To switch from one SDN plug-in to another, admin needs to:

. Change the *networkPluginName* parameter in both master and node configuration files.
ifdef::openshift-origin[]
. Restart origin-master service on master and origin-node service on all nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart atomic-openshift-master service on master and atomic-openshift-node service on all nodes.
endif::[]

When switching from ovssubnet to multitenant plug-in, all the existing projects in the cluster
will be fully isolated (assigned unique VNIDs). Admin can choose to join or unisolate project
networks using link:../../admin_guide/pod_network.html[oadm pod-network] CLI command.
endif::[]
