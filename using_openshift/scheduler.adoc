= Scheduler
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview
The Kubernetes pod scheduler is responsible for scheduling new pods onto minions within the cluster.  It reads data from the pod and tries to find a minion that is a good fit based on configured policies.  It is completely independent and exists as a standalone/pluggable solution.  It does not modify the pod and just creates a binding for the pod that ties the pod to the particular minion. 

== Generic Scheduler
The existing generic scheduler is the default platform-provided scheduler "engine" that selects a minion to host the pod in a 3-step operation: 

=== Step 1: Filter the minions 
The available minions are filtered based on constraints/requirements specified.  This is done by running each of the the minion through the list of filter functions, called 'predicates'.

=== Step 2: Prioritize the filtered list of minions
This is achieved by passing each minion through a series of 'priority' functions that assign it a score between 0 - 10, with 0 indicating a bad fit and 10 indicating a good fit to host the pod.  The scheduler configuration can also take in a simple "weight" (positive numeric value) for each priority function.  The minion score provided by each priority function is multiplied by the "weight" (default weight is 1) and then combined by just adding the scores for each minion provided by all the priority functions.  This weight attribute can be used by administrators to give higher importance to some priority functions. 

=== Step 3: Select the best fit minion
The minions are sorted based on their scores and minion with the highest score is selected to host the pod.  If multiple minions have the same high score, then one of them is selected at random. 


== Algorithm Providers
There are several predicate and priority functions that have been provided within Kubernetes.  'Providers' can be created that specify a pre-determined combination of the predicates and priority functions along with their weights.  A few providers, including a default provider, have been created for providing the scheduler configuration.  In addition, a custom set of predicates and priority functions can be selected to configure the scheduler. 


== High Level Goals
One of the high level goals of scheduling within OpenShift is to support flexible affinity and anti-affinity policies.  In order to achieve this, we need to:

1. allow admins to define multiple topological levels for their infrastructure (minions)
1. allow admins to specify affinity and anti-affinity at each of these levels for scheduling pods (addressed by the PR)

The former is done by specifying simple labels on the minion resources (eg: region = r1, zone = z1, rack = s1). These label names have no particular meaning and admins are free to name their infrastructure levels anything (eg, city/building/room). The latter is achieved by configuring the scheduler with the selection of a custom set of predicates and priority functions that define the desired affinity and anti-affinity rules. 


== Use Cases
Administrators should be able to select scheduling policies/configuration that allow them to define affinity and anti-affinity rules for pods within the service based on the infrastructure topology.  Common use cases are:

1. I should be able to define any number of nested levels (if not completely flexible, then at least 3) for my infrastructure topology (eg. regions/zones/racks)
1. I should be able to define affinity and anti-affinity rules for those nested levels in any combination
1. At a minimum, the rules should handle:
  * Affinity -> Affnity -> Anti-Affinity
  * Affinity -> Anti-Affnity -> Anti-Affinity
  * Affinity -> Anti-Affinity
  * Affinity -> Affnity
  * Anti-Affnity -> Anti-Affinity

=== Service Affinity
This is implemented by a service affinity predicate that filters out minions that do not belong to the specified topological level.  This predicate takes in a label (in the example below, its zone) and ensures affinity within the same zone for pods belonging to the same service.  If the pod specifies a particular "zone" in its NodeSelector, then that zone is the one where the pod is scheduled. If the pod does not specify the "zone" NodeSelector, then the first pod can be placed in any zone based on availability and all subsequent pods of the service will be scheduled on minions within the same zone.

=== Service Anti-Affinity
This is implemented service anti-affinity priority function that gives a higher score to minions with the least concentration of pods. This priority function takes a label (in the example below, its "rack") and ensures a good spread of the pods belonging to the same service across the different racks.  Multiple priority functions can be combined and weights can be given to each to impact the prioritization.  The default weight is 1 and in the example below, the priority function has been given a weight of 2.

== Example code
=== API
----
func RegisterFitPredicate(name string, predicate algorithm.FitPredicate) string
func RegisterPriorityFunction(name string, function algorithm.PriorityFunction, weight int) string
----

=== Sample
----
factory.RegisterFitPredicate("ZoneAffinity", algorithm.NewServiceAffinityPredicate(factory.PodLister, factory.ServiceLister, factory.MinionLister, []string{"zone"}))
factory.RegisterPriorityFunction("RackAntiAffinity", algorithm.NewServiceAntiAffinityPriority(factory.ServiceLister, "rack"), 2)
----

== Remaining Work
Lastly, work is ongoing to allow administrators to specify the predicates/priorities/weights for configuring the scheduler policies. 
----
Trello Card --> https://trello.com/c/DdNjfRU4
Upstream Issue --> https://github.com/GoogleCloudPlatform/kubernetes/issues/4303
----
